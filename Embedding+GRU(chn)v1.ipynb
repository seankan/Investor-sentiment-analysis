{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample_data/sample_data_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>docid</th>\n",
       "      <th>stock_code</th>\n",
       "      <th>author</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.020000e+14</td>\n",
       "      <td>600340.sh</td>\n",
       "      <td>狼子會投</td>\n",
       "      <td>36元成本，七萬股，目標價50元</td>\n",
       "      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n",
       "      <td>36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.020000e+14</td>\n",
       "      <td>600340.sh</td>\n",
       "      <td>整齊皓月</td>\n",
       "      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？</td>\n",
       "      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n",
       "      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？華夏幸福36幹進來的，幾乎滿倉了，後...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.020000e+14</td>\n",
       "      <td>600340.sh</td>\n",
       "      <td>思理紅嫩</td>\n",
       "      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？</td>\n",
       "      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n",
       "      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？華夏幸福36幹進來的，幾乎滿倉了，後...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.020000e+14</td>\n",
       "      <td>600211.sh</td>\n",
       "      <td>再見理想123</td>\n",
       "      <td>從集中持股到資產配置，可這樣走</td>\n",
       "      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n",
       "      <td>從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.020000e+14</td>\n",
       "      <td>002718.sz</td>\n",
       "      <td>genglp</td>\n",
       "      <td>上市公司當下賣房屬於</td>\n",
       "      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n",
       "      <td>上市公司當下賣房屬於上市公司當下賣房屬於一是基於但由於業績下滑</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         docid stock_code   author                      headline  \\\n",
       "0           0  2.020000e+14  600340.sh     狼子會投              36元成本，七萬股，目標價50元   \n",
       "1           1  2.020000e+14  600340.sh     整齊皓月  華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？   \n",
       "2           2  2.020000e+14  600340.sh     思理紅嫩  華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？   \n",
       "3           3  2.020000e+14  600211.sh  再見理想123               從集中持股到資產配置，可這樣走   \n",
       "4           4  2.020000e+14  002718.sz   genglp                    上市公司當下賣房屬於   \n",
       "\n",
       "                                             content  \\\n",
       "0  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n",
       "1  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n",
       "2  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n",
       "3  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n",
       "4  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n",
       "\n",
       "                                       clean_content  label  \n",
       "0   36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元      1  \n",
       "1  華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？華夏幸福36幹進來的，幾乎滿倉了，後...      1  \n",
       "2  華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？華夏幸福36幹進來的，幾乎滿倉了，後...      1  \n",
       "3  從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又...      0  \n",
       "4                    上市公司當下賣房屬於上市公司當下賣房屬於一是基於但由於業績下滑      0  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skipped due to data cleansing in precvious work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又當如何立足！'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_content'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(str(data['clean_content'][100])).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import codecs\n",
    "from langconv import * # convert Traditional Chinese characters to Simplified Chinese characters\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "籃球\n",
      "篮球\n"
     ]
    }
   ],
   "source": [
    "line = \"籃球\"\n",
    "print(line)\n",
    "\n",
    "line = Converter('zh-hans').convert(line)\n",
    "print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元\n",
       "1    華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？華夏幸福36幹進來的，幾乎滿倉了，後...\n",
       "2    華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？華夏幸福36幹進來的，幾乎滿倉了，後...\n",
       "3    從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又...\n",
       "4                      上市公司當下賣房屬於上市公司當下賣房屬於一是基於但由於業績下滑\n",
       "Name: clean_content, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.DataFrame\n",
    "file = data['clean_content']\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(file)):\n",
    "    file[i] =  Converter('zh-hans').convert(str(file[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     36元成本，七万股，目标价50元36元成本，七万股，目标价50元36元成本，七万股，目标价50元\n",
       "1    华夏幸福36干进来的，几乎满仓了，后市如何？下周要撤不？华夏幸福36干进来的，几乎满仓了，后...\n",
       "2    华夏幸福36干进来的，几乎满仓了，后市如何，大神指点下？华夏幸福36干进来的，几乎满仓了，后...\n",
       "3    从集中持股到资产配置，可这样走从集中持股到资产配置，可这样走周四大幅下杀是否另有玄机，周五又...\n",
       "4                      上市公司当下卖房属于上市公司当下卖房属于一是基于但由于业绩下滑\n",
       "Name: clean_content, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Orginal==:\n",
      "36元成本，七万股，目标价50元36元成本，七万股，目标价50元36元成本，七万股，目标价50元\n",
      "==Tokenized==\tToken count:30\n",
      "36 元 成本 ， 七 万股 ， 目标价 50 元 36 元 成本 ， 七 万股 ， 目标价 50 元 36 元 成本 ， 七 万股 ， 目标价 50 元\n",
      "==Stop Words Removed==\tToken count:21\n",
      "36 元 成本 万股 目标价 50 元 36 元 成本 万股 目标价 50 元 36 元 成本 万股 目标价 50 元\n"
     ]
    }
   ],
   "source": [
    "filename = file[0]\n",
    "text= filename\n",
    "text = text.replace(\"\\n\", \"\")\n",
    "text = text.replace(\"\\r\", \"\")\n",
    "print(\"==Orginal==:\\n\\r{}\".format(text))\n",
    "    \n",
    "stopwords = [ line.rstrip() for line in codecs.open('./sample_data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "final =[]\n",
    "seg_list = list(seg_list)\n",
    "for seg in seg_list:\n",
    "    if seg not in stopwords:\n",
    "        final.append(seg)\n",
    "print(\"==Tokenized==\\tToken count:{}\\n\\r{}\".format(len(seg_list),\" \".join(seg_list)))\n",
    "print(\"==Stop Words Removed==\\tToken count:{}\\n\\r{}\".format(len(final),\" \".join(final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(0,1000):\n",
    "    if data['label'][i] == '2':\n",
    "#     with codecs.open(filename, \"rb\") as doc_file:\n",
    "#         for line in doc_file:\n",
    "#             try:\n",
    "#                 line = line.decode(\"GB2312\")\n",
    "#             except:\n",
    "#                 continue\n",
    "        text+=Converter('zh-hans').convert(data['cleaned_content'][i])# Convert from traditional to simplified Chinese\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"pos\"))\n",
    "\n",
    "    elif data['label'][i] == '0':\n",
    "#     with codecs.open(filename, \"rb\") as doc_file:\n",
    "#         for line in doc_file:\n",
    "#             try:\n",
    "#                 line = line.decode(\"GB2312\")\n",
    "#             except:\n",
    "#                 continue\n",
    "        text+=Converter('zh-hans').convert(data['cleaned_content'][i])# Convert from traditional to simplified Chinese\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"neg\"))\n",
    "    \n",
    "    elif data['label'][i] == '1':\n",
    "        text+=Converter('zh-hans').convert(data['cleaned_content'][i])# Convert from traditional to simplified Chinese\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"neu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "final=[]\n",
    "for i in range(0,1000):\n",
    "    seg_list = jieba.cut(file[i], cut_all=False)\n",
    "    temp =[]\n",
    "    seg_list = list(seg_list)\n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            temp.append(seg)\n",
    "    final.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is:  1133\n",
      "80% cover length up to:  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "h = sorted([len(sentence) for sentence in final])\n",
    "maxLength = h[int(len(h) * 0.80)]\n",
    "print(\"Max length is: \",h[len(h)-1])\n",
    "print(\"80% cover length up to: \",maxLength)\n",
    "h = h[:5000]\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "\n",
    "pl.plot(h,fit,'-o')\n",
    "pl.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict = {}\n",
    "for sent in final:\n",
    "    for word in sent:\n",
    "        wordDict[word] = 1\n",
    "wordList = list(wordDict.keys())\n",
    "wordDict = {}\n",
    "for index, word in enumerate(wordList):\n",
    "    wordDict[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHotTable = np.zeros(shape=(len(final), len(wordList)))\n",
    "for index, sent in enumerate(final):\n",
    "    for word in sent:\n",
    "        oneHotTable[index][wordDict[word]] = 1\n",
    "oneHotTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocab_size: 7530\n"
     ]
    }
   ],
   "source": [
    "totalX = [\" \".join(wordslist) for wordslist in final]  # Keras Tokenizer expect the words tokens to be seperated by space \n",
    "input_tokenizer = Tokenizer(30000) # Initial vocab size\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input vocab_size:\",vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX), maxlen=maxLength))\n",
    "__pickleStuff(\"./sample_data/input_tokenizer_chinese.p\", input_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 31)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer = Tokenizer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalY = to_categorical(y, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimen = totalY.shape[1] # which is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tag = ['neg','neu','pos'] # either [\"neg\",\"pos\"] or [\"pos\",\"neg\"]\n",
    "metaData = {\"maxLength\":maxLength,\"vocab_size\":vocab_size,\"output_dimen\":output_dimen,\"sentiment_tag\":sentiment_tag}\n",
    "__pickleStuff(\"./sample_data/meta_sentiment_chinese.p\", metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 31, 256)           1927680   \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 31, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,716,419\n",
      "Trainable params: 2,716,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/20\n",
      "700/700 [==============================] - 6s 8ms/step - loss: 1.0035 - accuracy: 0.5800 - val_loss: 1.1710 - val_accuracy: 0.3800\n",
      "Epoch 2/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.9321 - accuracy: 0.6171 - val_loss: 1.1868 - val_accuracy: 0.3800\n",
      "Epoch 3/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.9054 - accuracy: 0.6257 - val_loss: 1.2247 - val_accuracy: 0.3800\n",
      "Epoch 4/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.8830 - accuracy: 0.6186 - val_loss: 1.1984 - val_accuracy: 0.3800\n",
      "Epoch 5/20\n",
      "700/700 [==============================] - 5s 7ms/step - loss: 0.8650 - accuracy: 0.6271 - val_loss: 1.1708 - val_accuracy: 0.4000\n",
      "Epoch 6/20\n",
      "700/700 [==============================] - 5s 7ms/step - loss: 0.7786 - accuracy: 0.6914 - val_loss: 1.2227 - val_accuracy: 0.4000\n",
      "Epoch 7/20\n",
      "700/700 [==============================] - 5s 6ms/step - loss: 0.7144 - accuracy: 0.7129 - val_loss: 1.2509 - val_accuracy: 0.4033949 - \n",
      "Epoch 8/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.6890 - accuracy: 0.7129 - val_loss: 1.3700 - val_accuracy: 0.4233\n",
      "Epoch 9/20\n",
      "700/700 [==============================] - 5s 6ms/step - loss: 0.6493 - accuracy: 0.7371 - val_loss: 1.4056 - val_accuracy: 0.4500\n",
      "Epoch 10/20\n",
      "700/700 [==============================] - 5s 6ms/step - loss: 0.5829 - accuracy: 0.7557 - val_loss: 1.4915 - val_accuracy: 0.4433\n",
      "Epoch 11/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.5348 - accuracy: 0.7843 - val_loss: 1.5096 - val_accuracy: 0.4133\n",
      "Epoch 12/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.5170 - accuracy: 0.7871 - val_loss: 2.0233 - val_accuracy: 0.4367\n",
      "Epoch 13/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.4748 - accuracy: 0.7900 - val_loss: 1.8089 - val_accuracy: 0.4367\n",
      "Epoch 14/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.4291 - accuracy: 0.8214 - val_loss: 2.1714 - val_accuracy: 0.4100\n",
      "Epoch 15/20\n",
      "700/700 [==============================] - 5s 6ms/step - loss: 0.3896 - accuracy: 0.8543 - val_loss: 1.9915 - val_accuracy: 0.4300\n",
      "Epoch 16/20\n",
      "700/700 [==============================] - 5s 6ms/step - loss: 0.4232 - accuracy: 0.8543 - val_loss: 1.9448 - val_accuracy: 0.4033\n",
      "Epoch 17/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.3945 - accuracy: 0.8457 - val_loss: 2.3409 - val_accuracy: 0.4433\n",
      "Epoch 18/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.3589 - accuracy: 0.8700 - val_loss: 1.7025 - val_accuracy: 0.4300\n",
      "Epoch 19/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.3375 - accuracy: 0.8771 - val_loss: 1.9901 - val_accuracy: 0.4333\n",
      "Epoch 20/20\n",
      "700/700 [==============================] - 4s 6ms/step - loss: 0.3143 - accuracy: 0.8900 - val_loss: 1.9324 - val_accuracy: 0.4267\n",
      "Saved model!\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,input_length = maxLength))\n",
    "# Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "# Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "# The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "model.add(Dense(output_dimen, activation='softmax'))\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "# tbCallBack = TensorBoard(log_dir='./Graph/sentiment_chinese', histogram_freq=0,\n",
    "#                             write_graph=True, write_images=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(totalX, totalY, validation_split=0.3, batch_size=32, epochs=20, verbose=1, )\n",
    "model.save('./sample_data/sentiment_chinese_model.HDF5')\n",
    "\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "sentiment_tag = None\n",
    "maxLength = None\n",
    "def loadModel():\n",
    "    global model, sentiment_tag, maxLength\n",
    "    metaData = __loadStuff(\"./sample_data/meta_sentiment_chinese.p\")\n",
    "    maxLength = metaData.get(\"maxLength\")\n",
    "    vocab_size = metaData.get(\"vocab_size\")\n",
    "    output_dimen = metaData.get(\"output_dimen\")\n",
    "    sentiment_tag = metaData.get(\"sentiment_tag\")\n",
    "    embedding_dim = 256\n",
    "    if model is None:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=maxLength))\n",
    "        # Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "        # All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "        model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "        # Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "        model.add(GRU(256, dropout=0.9))\n",
    "        # The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "        model.add(Dense(output_dimen, activation='softmax'))\n",
    "        # We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.load_weights('./sample_data/sentiment_chinese_model.HDF5')\n",
    "        model.summary()\n",
    "    print(\"Model weights loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeatures(text):\n",
    "    text=Converter('zh-hans').convert(text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\") \n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    text = \" \".join(seg_list)\n",
    "    textArray = [text]\n",
    "    input_tokenizer_load = __loadStuff(\"./sample_data/input_tokenizer_chinese.p\")\n",
    "    textArray = np.array(pad_sequences(input_tokenizer_load.texts_to_sequences(textArray), maxlen=maxLength))\n",
    "    return textArray\n",
    "def predictResult(text):\n",
    "    if model is None:\n",
    "        print(\"Please run \\\"loadModel\\\" first.\")\n",
    "        return None\n",
    "    features = findFeatures(text)\n",
    "    predicted = model.predict(features)[0] # we have only one sentence to predict, so take index 0\n",
    "    predicted = np.array(predicted)\n",
    "    probab = predicted.max()\n",
    "    predition = sentiment_tag[predicted.argmax()]\n",
    "    return predition, probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 31, 256)           1927680   \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 31, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,716,419\n",
      "Trainable params: 2,716,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model weights loaded!\n"
     ]
    }
   ],
   "source": [
    "loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neu', 0.9980044)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"华夏幸福36干进来的，几乎满仓了，后市如何？下周要撤不？华夏幸福36干进来的，几乎满仓了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stock recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201705013063592.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['docid'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201705013065375.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['docid'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with signal and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
