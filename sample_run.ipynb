{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Reshape, Conv2D, Activation, MaxPooling2D, Flatten\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample_data/sample_data_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <html>\\n <meta http-equiv=\"Content-Type\" conte...\n",
       "1    <html>\\n <meta http-equiv=\"Content-Type\" conte...\n",
       "2    <html>\\n <meta http-equiv=\"Content-Type\" conte...\n",
       "3    <html>\\n <meta http-equiv=\"Content-Type\" conte...\n",
       "4    <html>\\n <meta http-equiv=\"Content-Type\" conte...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skipped due to data cleansing in precvious work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元\n",
       "1    華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？華夏幸福36幹進來的，幾乎滿倉了，後...\n",
       "2    華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？華夏幸福36幹進來的，幾乎滿倉了，後...\n",
       "3    從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又...\n",
       "4                      上市公司當下賣房屬於上市公司當下賣房屬於一是基於但由於業績下滑\n",
       "Name: clean_content, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(str(data['clean_content'][100])).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import jieba\n",
    "import codecs\n",
    "from langconv import * # convert Traditional Chinese characters to Simplified Chinese characters\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Reshape, Conv2D, Activation, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "籃球\n",
      "篮球\n"
     ]
    }
   ],
   "source": [
    "line = \"籃球\"\n",
    "print(line)\n",
    "\n",
    "line = Converter('zh-hans').convert(line)\n",
    "print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元\n",
       "1    華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？華夏幸福36幹進來的，幾乎滿倉了，後...\n",
       "2    華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？華夏幸福36幹進來的，幾乎滿倉了，後...\n",
       "3    從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又...\n",
       "4                      上市公司當下賣房屬於上市公司當下賣房屬於一是基於但由於業績下滑\n",
       "Name: clean_content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.DataFrame\n",
    "file = data['clean_content']\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(file)):\n",
    "    file[i] =  Converter('zh-hans').convert(str(file[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     36元成本，七万股，目标价50元36元成本，七万股，目标价50元36元成本，七万股，目标价50元\n",
       "1    华夏幸福36干进来的，几乎满仓了，后市如何？下周要撤不？华夏幸福36干进来的，几乎满仓了，后...\n",
       "2    华夏幸福36干进来的，几乎满仓了，后市如何，大神指点下？华夏幸福36干进来的，几乎满仓了，后...\n",
       "3    从集中持股到资产配置，可这样走从集中持股到资产配置，可这样走周四大幅下杀是否另有玄机，周五又...\n",
       "4                      上市公司当下卖房属于上市公司当下卖房属于一是基于但由于业绩下滑\n",
       "Name: clean_content, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Orginal==:\n",
      "36元成本，七万股，目标价50元36元成本，七万股，目标价50元36元成本，七万股，目标价50元\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\Sean\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.088 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Tokenized==\tToken count:30\n",
      "36 元 成本 ， 七 万股 ， 目标价 50 元 36 元 成本 ， 七 万股 ， 目标价 50 元 36 元 成本 ， 七 万股 ， 目标价 50 元\n",
      "==Stop Words Removed==\tToken count:21\n",
      "36 元 成本 万股 目标价 50 元 36 元 成本 万股 目标价 50 元 36 元 成本 万股 目标价 50 元\n"
     ]
    }
   ],
   "source": [
    "filename = file[0]\n",
    "text= filename\n",
    "text = text.replace(\"\\n\", \"\")\n",
    "text = text.replace(\"\\r\", \"\")\n",
    "print(\"==Orginal==:\\n\\r{}\".format(text))\n",
    "    \n",
    "stopwords = [ line.rstrip() for line in codecs.open('./sample_data/chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "final =[]\n",
    "seg_list = list(seg_list)\n",
    "for seg in seg_list:\n",
    "    if seg not in stopwords:\n",
    "        final.append(seg)\n",
    "print(\"==Tokenized==\\tToken count:{}\\n\\r{}\".format(len(seg_list),\" \".join(seg_list)))\n",
    "print(\"==Stop Words Removed==\\tToken count:{}\\n\\r{}\".format(len(final),\" \".join(final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(0,1000):\n",
    "    if data['label'][i] == '2':\n",
    "#     with codecs.open(filename, \"rb\") as doc_file:\n",
    "#         for line in doc_file:\n",
    "#             try:\n",
    "#                 line = line.decode(\"GB2312\")\n",
    "#             except:\n",
    "#                 continue\n",
    "        text+=Converter('zh-hans').convert(data['cleaned_content'][i])# Convert from traditional to simplified Chinese\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"pos\"))\n",
    "\n",
    "    elif data['label'][i] == '0':\n",
    "#     with codecs.open(filename, \"rb\") as doc_file:\n",
    "#         for line in doc_file:\n",
    "#             try:\n",
    "#                 line = line.decode(\"GB2312\")\n",
    "#             except:\n",
    "#                 continue\n",
    "        text+=Converter('zh-hans').convert(data['cleaned_content'][i])# Convert from traditional to simplified Chinese\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"neg\"))\n",
    "    \n",
    "    elif data['label'][i] == '1':\n",
    "        text+=Converter('zh-hans').convert(data['cleaned_content'][i])# Convert from traditional to simplified Chinese\n",
    "\n",
    "        text = text.replace(\"\\n\", \"\")\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "        documents.append((text, \"neu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "final=[]\n",
    "for i in range(0,1000):\n",
    "    seg_list = jieba.cut(file[i], cut_all=False)\n",
    "    temp =[]\n",
    "    seg_list = list(seg_list)\n",
    "    for seg in seg_list:\n",
    "        if seg not in stopwords:\n",
    "            temp.append(seg)\n",
    "    final.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is:  1133\n",
      "99% cover length up to:  723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "h = sorted([len(sentence) for sentence in final])\n",
    "maxLength = h[int(len(h) * 0.99)]\n",
    "print(\"Max length is: \",h[len(h)-1])\n",
    "print(\"99% cover length up to: \",maxLength)\n",
    "h = h[:5000]\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "\n",
    "pl.plot(h,fit,'-o')\n",
    "pl.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict = {}\n",
    "for sent in final:\n",
    "    for word in sent:\n",
    "        wordDict[word] = 1\n",
    "wordList = list(wordDict.keys())\n",
    "wordDict = {}\n",
    "for index, word in enumerate(wordList):\n",
    "    wordDict[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHotTable = np.zeros(shape=(len(final), len(wordList)))\n",
    "for index, sent in enumerate(final):\n",
    "    for word in sent:\n",
    "        oneHotTable[index][wordDict[word]] = 1\n",
    "oneHotTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = Tokenizer(30000)\n",
    "input_tokenizer.fit_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocab_size: 7530\n"
     ]
    }
   ],
   "source": [
    "totalX = [\" \".join(wordslist) for wordslist in final]  # Keras Tokenizer expect the words tokens to be seperated by space \n",
    "input_tokenizer = Tokenizer(30000) # Initial vocab size\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input vocab_size:\",vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX), maxlen=1000))\n",
    "__pickleStuff(\"./sample_data/input_tokenizer_chinese.p\", input_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer = Tokenizer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data[\"label\"]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3     -1\n",
       "4     -1\n",
       "5      0\n",
       "6      0\n",
       "7     -1\n",
       "8      0\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     0\n",
       "13     0\n",
       "14     1\n",
       "15     0\n",
       "16    -1\n",
       "17    -1\n",
       "18     1\n",
       "19    -1\n",
       "20     1\n",
       "21     0\n",
       "22     0\n",
       "23    -1\n",
       "24    -1\n",
       "25     1\n",
       "26    -1\n",
       "27     0\n",
       "28     1\n",
       "29     0\n",
       "      ..\n",
       "970   -1\n",
       "971    1\n",
       "972    0\n",
       "973    1\n",
       "974    0\n",
       "975   -1\n",
       "976   -1\n",
       "977    1\n",
       "978   -1\n",
       "979    1\n",
       "980    1\n",
       "981    1\n",
       "982   -1\n",
       "983    0\n",
       "984   -1\n",
       "985    1\n",
       "986    1\n",
       "987    1\n",
       "988   -1\n",
       "989   -1\n",
       "990    0\n",
       "991   -1\n",
       "992    1\n",
       "993    1\n",
       "994    1\n",
       "995    1\n",
       "996    0\n",
       "997    1\n",
       "998    0\n",
       "999    0\n",
       "Name: label, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalY = np_utils.to_categorical(y,num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dimen = totalY.shape[1] # which is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tag = ['neg','neu','pos'] # either [\"neg\",\"pos\"] or [\"pos\",\"neg\"]\n",
    "metaData = {\"maxLength\":maxLength,\"vocab_size\":vocab_size,\"output_dimen\":output_dimen,\"sentiment_tag\":sentiment_tag}\n",
    "__pickleStuff(\"./sample_data/meta_sentiment_chinese.p\", metaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1732,  185,   11],\n",
       "       [   0,    0,    0, ...,  233,   47, 1185],\n",
       "       [   0,    0,    0, ...,  233, 2183, 1186],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    2, 1084,   62],\n",
       "       [   0,    0,    0, ...,   32,  238,  321],\n",
       "       [   0,    0,    0, ...,  407, 1172,   74]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/500\n",
      "900/900 [==============================] - 2s 3ms/sample - loss: 1.0538 - accuracy: 0.4733 - val_loss: 1.2124 - val_accuracy: 0.2900\n",
      "Epoch 2/500\n",
      "900/900 [==============================] - 0s 337us/sample - loss: 0.9867 - accuracy: 0.5644 - val_loss: 1.2874 - val_accuracy: 0.2900\n",
      "Epoch 3/500\n",
      "900/900 [==============================] - 0s 340us/sample - loss: 0.9762 - accuracy: 0.5744 - val_loss: 1.2865 - val_accuracy: 0.2900\n",
      "Epoch 4/500\n",
      "900/900 [==============================] - 0s 386us/sample - loss: 0.9710 - accuracy: 0.5756 - val_loss: 1.2486 - val_accuracy: 0.2900\n",
      "Epoch 5/500\n",
      "900/900 [==============================] - 0s 340us/sample - loss: 0.9560 - accuracy: 0.5856 - val_loss: 1.2291 - val_accuracy: 0.2900\n",
      "Epoch 6/500\n",
      "900/900 [==============================] - 0s 345us/sample - loss: 0.9477 - accuracy: 0.5811 - val_loss: 1.2130 - val_accuracy: 0.3000\n",
      "Epoch 7/500\n",
      "900/900 [==============================] - 0s 342us/sample - loss: 0.9489 - accuracy: 0.5811 - val_loss: 1.2319 - val_accuracy: 0.2900\n",
      "Epoch 8/500\n",
      "900/900 [==============================] - 0s 353us/sample - loss: 0.9475 - accuracy: 0.5800 - val_loss: 1.2110 - val_accuracy: 0.3000\n",
      "Epoch 9/500\n",
      "900/900 [==============================] - 0s 375us/sample - loss: 0.9389 - accuracy: 0.5922 - val_loss: 1.2003 - val_accuracy: 0.3200\n",
      "Epoch 10/500\n",
      "900/900 [==============================] - 0s 475us/sample - loss: 0.9443 - accuracy: 0.5856 - val_loss: 1.2460 - val_accuracy: 0.2900\n",
      "Epoch 11/500\n",
      "900/900 [==============================] - 0s 440us/sample - loss: 0.9439 - accuracy: 0.5867 - val_loss: 1.2465 - val_accuracy: 0.3000\n",
      "Epoch 12/500\n",
      "900/900 [==============================] - 0s 346us/sample - loss: 0.9293 - accuracy: 0.5822 - val_loss: 1.2658 - val_accuracy: 0.2900\n",
      "Epoch 13/500\n",
      "900/900 [==============================] - 0s 338us/sample - loss: 0.9442 - accuracy: 0.5767 - val_loss: 1.2749 - val_accuracy: 0.2900\n",
      "Epoch 14/500\n",
      "900/900 [==============================] - 0s 341us/sample - loss: 0.9372 - accuracy: 0.5922 - val_loss: 1.2445 - val_accuracy: 0.3000\n",
      "Epoch 15/500\n",
      "900/900 [==============================] - 0s 342us/sample - loss: 0.9350 - accuracy: 0.5722 - val_loss: 1.2349 - val_accuracy: 0.3000\n",
      "Epoch 16/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.9364 - accuracy: 0.5767 - val_loss: 1.2568 - val_accuracy: 0.2800\n",
      "Epoch 17/500\n",
      "900/900 [==============================] - 0s 337us/sample - loss: 0.9382 - accuracy: 0.5944 - val_loss: 1.2633 - val_accuracy: 0.2900\n",
      "Epoch 18/500\n",
      "900/900 [==============================] - 0s 408us/sample - loss: 0.9269 - accuracy: 0.5900 - val_loss: 1.2631 - val_accuracy: 0.3100\n",
      "Epoch 19/500\n",
      "900/900 [==============================] - 0s 319us/sample - loss: 0.9193 - accuracy: 0.5811 - val_loss: 1.2701 - val_accuracy: 0.3100\n",
      "Epoch 20/500\n",
      "900/900 [==============================] - 0s 325us/sample - loss: 0.9154 - accuracy: 0.5956 - val_loss: 1.2409 - val_accuracy: 0.3000\n",
      "Epoch 21/500\n",
      "900/900 [==============================] - 0s 338us/sample - loss: 0.9095 - accuracy: 0.5900 - val_loss: 1.2652 - val_accuracy: 0.3300\n",
      "Epoch 22/500\n",
      "900/900 [==============================] - 0s 448us/sample - loss: 0.9259 - accuracy: 0.5856 - val_loss: 1.2439 - val_accuracy: 0.3400\n",
      "Epoch 23/500\n",
      "900/900 [==============================] - 0s 349us/sample - loss: 0.9060 - accuracy: 0.5956 - val_loss: 1.2526 - val_accuracy: 0.3000\n",
      "Epoch 24/500\n",
      "900/900 [==============================] - 0s 336us/sample - loss: 0.9167 - accuracy: 0.5878 - val_loss: 1.2680 - val_accuracy: 0.3200\n",
      "Epoch 25/500\n",
      "900/900 [==============================] - 0s 324us/sample - loss: 0.8978 - accuracy: 0.6011 - val_loss: 1.2263 - val_accuracy: 0.3300\n",
      "Epoch 26/500\n",
      "900/900 [==============================] - 0s 339us/sample - loss: 0.8931 - accuracy: 0.6033 - val_loss: 1.2720 - val_accuracy: 0.3200\n",
      "Epoch 27/500\n",
      "900/900 [==============================] - 0s 315us/sample - loss: 0.8879 - accuracy: 0.6000 - val_loss: 1.2771 - val_accuracy: 0.3600\n",
      "Epoch 28/500\n",
      "900/900 [==============================] - 0s 322us/sample - loss: 0.9050 - accuracy: 0.5867 - val_loss: 1.2583 - val_accuracy: 0.3300\n",
      "Epoch 29/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.8910 - accuracy: 0.6111 - val_loss: 1.2629 - val_accuracy: 0.3400\n",
      "Epoch 30/500\n",
      "900/900 [==============================] - 0s 311us/sample - loss: 0.8706 - accuracy: 0.6189 - val_loss: 1.2737 - val_accuracy: 0.3300\n",
      "Epoch 31/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.8768 - accuracy: 0.6156 - val_loss: 1.2824 - val_accuracy: 0.3100\n",
      "Epoch 32/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.8736 - accuracy: 0.6000 - val_loss: 1.2822 - val_accuracy: 0.3100\n",
      "Epoch 33/500\n",
      "900/900 [==============================] - 0s 322us/sample - loss: 0.8565 - accuracy: 0.6200 - val_loss: 1.2838 - val_accuracy: 0.3300\n",
      "Epoch 34/500\n",
      "900/900 [==============================] - 0s 314us/sample - loss: 0.8670 - accuracy: 0.6167 - val_loss: 1.3440 - val_accuracy: 0.2900\n",
      "Epoch 35/500\n",
      "900/900 [==============================] - 0s 334us/sample - loss: 0.8935 - accuracy: 0.6078 - val_loss: 1.3393 - val_accuracy: 0.2900\n",
      "Epoch 36/500\n",
      "900/900 [==============================] - 0s 328us/sample - loss: 0.8959 - accuracy: 0.5933 - val_loss: 1.3281 - val_accuracy: 0.2900\n",
      "Epoch 37/500\n",
      "900/900 [==============================] - 0s 322us/sample - loss: 0.8546 - accuracy: 0.6233 - val_loss: 1.3033 - val_accuracy: 0.3100\n",
      "Epoch 38/500\n",
      "900/900 [==============================] - 0s 319us/sample - loss: 0.8768 - accuracy: 0.6211 - val_loss: 1.2895 - val_accuracy: 0.3300\n",
      "Epoch 39/500\n",
      "900/900 [==============================] - 0s 309us/sample - loss: 0.8505 - accuracy: 0.6233 - val_loss: 1.2522 - val_accuracy: 0.3500\n",
      "Epoch 40/500\n",
      "900/900 [==============================] - 0s 310us/sample - loss: 0.8533 - accuracy: 0.6189 - val_loss: 1.2818 - val_accuracy: 0.3200\n",
      "Epoch 41/500\n",
      "900/900 [==============================] - 0s 311us/sample - loss: 0.8636 - accuracy: 0.6311 - val_loss: 1.3094 - val_accuracy: 0.2800\n",
      "Epoch 42/500\n",
      "900/900 [==============================] - 0s 325us/sample - loss: 0.8677 - accuracy: 0.6100 - val_loss: 1.3661 - val_accuracy: 0.2800\n",
      "Epoch 43/500\n",
      "900/900 [==============================] - 0s 326us/sample - loss: 0.8418 - accuracy: 0.6222 - val_loss: 1.3443 - val_accuracy: 0.3000\n",
      "Epoch 44/500\n",
      "900/900 [==============================] - 0s 309us/sample - loss: 0.8435 - accuracy: 0.6289 - val_loss: 1.3084 - val_accuracy: 0.3100\n",
      "Epoch 45/500\n",
      "900/900 [==============================] - 0s 324us/sample - loss: 0.8478 - accuracy: 0.6100 - val_loss: 1.3300 - val_accuracy: 0.3200\n",
      "Epoch 46/500\n",
      "900/900 [==============================] - 0s 315us/sample - loss: 0.8270 - accuracy: 0.6467 - val_loss: 1.3727 - val_accuracy: 0.3000\n",
      "Epoch 47/500\n",
      "900/900 [==============================] - 0s 304us/sample - loss: 0.8089 - accuracy: 0.6378 - val_loss: 1.3828 - val_accuracy: 0.3100\n",
      "Epoch 48/500\n",
      "900/900 [==============================] - 0s 305us/sample - loss: 0.8097 - accuracy: 0.6489 - val_loss: 1.4246 - val_accuracy: 0.3100\n",
      "Epoch 49/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.8071 - accuracy: 0.6467 - val_loss: 1.4731 - val_accuracy: 0.3000\n",
      "Epoch 50/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.8078 - accuracy: 0.6422 - val_loss: 1.4460 - val_accuracy: 0.2900\n",
      "Epoch 51/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.7972 - accuracy: 0.6700 - val_loss: 1.3802 - val_accuracy: 0.3000\n",
      "Epoch 52/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.7942 - accuracy: 0.6767 - val_loss: 1.4357 - val_accuracy: 0.3100\n",
      "Epoch 53/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.7981 - accuracy: 0.6700 - val_loss: 1.3975 - val_accuracy: 0.3300\n",
      "Epoch 54/500\n",
      "900/900 [==============================] - 0s 305us/sample - loss: 0.7785 - accuracy: 0.6689 - val_loss: 1.4110 - val_accuracy: 0.3200\n",
      "Epoch 55/500\n",
      "900/900 [==============================] - 0s 314us/sample - loss: 0.7638 - accuracy: 0.6844 - val_loss: 1.3781 - val_accuracy: 0.3600\n",
      "Epoch 56/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.7844 - accuracy: 0.6422 - val_loss: 1.3783 - val_accuracy: 0.3600\n",
      "Epoch 57/500\n",
      "900/900 [==============================] - 0s 311us/sample - loss: 0.7647 - accuracy: 0.6711 - val_loss: 1.3691 - val_accuracy: 0.3200\n",
      "Epoch 58/500\n",
      "900/900 [==============================] - 0s 332us/sample - loss: 0.7690 - accuracy: 0.6678 - val_loss: 1.3999 - val_accuracy: 0.3500\n",
      "Epoch 59/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.7697 - accuracy: 0.6589 - val_loss: 1.4013 - val_accuracy: 0.3900\n",
      "Epoch 60/500\n",
      "900/900 [==============================] - 0s 327us/sample - loss: 0.7678 - accuracy: 0.6922 - val_loss: 1.4492 - val_accuracy: 0.3800\n",
      "Epoch 61/500\n",
      "900/900 [==============================] - 0s 311us/sample - loss: 0.7469 - accuracy: 0.6756 - val_loss: 1.4133 - val_accuracy: 0.3600\n",
      "Epoch 62/500\n",
      "900/900 [==============================] - 0s 311us/sample - loss: 0.7275 - accuracy: 0.6944 - val_loss: 1.4281 - val_accuracy: 0.3900\n",
      "Epoch 63/500\n",
      "900/900 [==============================] - 0s 335us/sample - loss: 0.7271 - accuracy: 0.7044 - val_loss: 1.4347 - val_accuracy: 0.3700\n",
      "Epoch 64/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.7228 - accuracy: 0.6967 - val_loss: 1.4359 - val_accuracy: 0.3800\n",
      "Epoch 65/500\n",
      "900/900 [==============================] - 0s 310us/sample - loss: 0.7189 - accuracy: 0.7033 - val_loss: 1.4611 - val_accuracy: 0.3600\n",
      "Epoch 66/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.7222 - accuracy: 0.7033 - val_loss: 1.4302 - val_accuracy: 0.4000\n",
      "Epoch 67/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.7010 - accuracy: 0.7178 - val_loss: 1.4100 - val_accuracy: 0.3700\n",
      "Epoch 68/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.7264 - accuracy: 0.6978 - val_loss: 1.4679 - val_accuracy: 0.3400\n",
      "Epoch 69/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.7019 - accuracy: 0.7100 - val_loss: 1.4689 - val_accuracy: 0.3300\n",
      "Epoch 70/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.6978 - accuracy: 0.6978 - val_loss: 1.4694 - val_accuracy: 0.3000\n",
      "Epoch 71/500\n",
      "900/900 [==============================] - 0s 339us/sample - loss: 0.6927 - accuracy: 0.7278 - val_loss: 1.5280 - val_accuracy: 0.3400\n",
      "Epoch 72/500\n",
      "900/900 [==============================] - 0s 309us/sample - loss: 0.6832 - accuracy: 0.7178 - val_loss: 1.5296 - val_accuracy: 0.3200\n",
      "Epoch 73/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.6717 - accuracy: 0.7378 - val_loss: 1.5121 - val_accuracy: 0.3400\n",
      "Epoch 74/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.6904 - accuracy: 0.7156 - val_loss: 1.5197 - val_accuracy: 0.3300\n",
      "Epoch 75/500\n",
      "900/900 [==============================] - 0s 309us/sample - loss: 0.6635 - accuracy: 0.7333 - val_loss: 1.5531 - val_accuracy: 0.3600\n",
      "Epoch 76/500\n",
      "900/900 [==============================] - 0s 386us/sample - loss: 0.6823 - accuracy: 0.7289 - val_loss: 1.5626 - val_accuracy: 0.3400\n",
      "Epoch 77/500\n",
      "900/900 [==============================] - 0s 472us/sample - loss: 0.6399 - accuracy: 0.7367 - val_loss: 1.5869 - val_accuracy: 0.3100\n",
      "Epoch 78/500\n",
      "900/900 [==============================] - 0s 385us/sample - loss: 0.6325 - accuracy: 0.7467 - val_loss: 1.5875 - val_accuracy: 0.3200\n",
      "Epoch 79/500\n",
      "900/900 [==============================] - 0s 449us/sample - loss: 0.6445 - accuracy: 0.7367 - val_loss: 1.5983 - val_accuracy: 0.3000\n",
      "Epoch 80/500\n",
      "900/900 [==============================] - 0s 327us/sample - loss: 0.6465 - accuracy: 0.7500 - val_loss: 1.5615 - val_accuracy: 0.3200\n",
      "Epoch 81/500\n",
      "900/900 [==============================] - 0s 390us/sample - loss: 0.6249 - accuracy: 0.7522 - val_loss: 1.5666 - val_accuracy: 0.3600\n",
      "Epoch 82/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.6162 - accuracy: 0.7578 - val_loss: 1.5271 - val_accuracy: 0.3300\n",
      "Epoch 83/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.5892 - accuracy: 0.7656 - val_loss: 1.5736 - val_accuracy: 0.3200\n",
      "Epoch 84/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.5920 - accuracy: 0.7722 - val_loss: 1.5845 - val_accuracy: 0.3300\n",
      "Epoch 85/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.6107 - accuracy: 0.7522 - val_loss: 1.5894 - val_accuracy: 0.3700\n",
      "Epoch 86/500\n",
      "900/900 [==============================] - 0s 349us/sample - loss: 0.5793 - accuracy: 0.7867 - val_loss: 1.5703 - val_accuracy: 0.4100\n",
      "Epoch 87/500\n",
      "900/900 [==============================] - 0s 357us/sample - loss: 0.5895 - accuracy: 0.7667 - val_loss: 1.5529 - val_accuracy: 0.3900\n",
      "Epoch 88/500\n",
      "900/900 [==============================] - 0s 350us/sample - loss: 0.5706 - accuracy: 0.8000 - val_loss: 1.5164 - val_accuracy: 0.4000\n",
      "Epoch 89/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.5582 - accuracy: 0.7756 - val_loss: 1.5648 - val_accuracy: 0.3700\n",
      "Epoch 90/500\n",
      "900/900 [==============================] - 0s 327us/sample - loss: 0.5663 - accuracy: 0.7767 - val_loss: 1.5573 - val_accuracy: 0.3700\n",
      "Epoch 91/500\n",
      "900/900 [==============================] - 0s 319us/sample - loss: 0.5491 - accuracy: 0.7867 - val_loss: 1.5492 - val_accuracy: 0.3900\n",
      "Epoch 92/500\n",
      "900/900 [==============================] - 0s 334us/sample - loss: 0.5122 - accuracy: 0.8011 - val_loss: 1.5835 - val_accuracy: 0.3900\n",
      "Epoch 93/500\n",
      "900/900 [==============================] - 0s 332us/sample - loss: 0.5701 - accuracy: 0.7767 - val_loss: 1.5901 - val_accuracy: 0.3600\n",
      "Epoch 94/500\n",
      "900/900 [==============================] - 0s 336us/sample - loss: 0.5381 - accuracy: 0.7833 - val_loss: 1.5326 - val_accuracy: 0.3900\n",
      "Epoch 95/500\n",
      "900/900 [==============================] - 0s 337us/sample - loss: 0.5433 - accuracy: 0.7889 - val_loss: 1.5640 - val_accuracy: 0.4100\n",
      "Epoch 96/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.5534 - accuracy: 0.7889 - val_loss: 1.6054 - val_accuracy: 0.3600\n",
      "Epoch 97/500\n",
      "900/900 [==============================] - 0s 335us/sample - loss: 0.5394 - accuracy: 0.7933 - val_loss: 1.5586 - val_accuracy: 0.3900\n",
      "Epoch 98/500\n",
      "900/900 [==============================] - 0s 325us/sample - loss: 0.5414 - accuracy: 0.7944 - val_loss: 1.6403 - val_accuracy: 0.3800\n",
      "Epoch 99/500\n",
      "900/900 [==============================] - 0s 322us/sample - loss: 0.5241 - accuracy: 0.8067 - val_loss: 1.6550 - val_accuracy: 0.3900\n",
      "Epoch 100/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.5162 - accuracy: 0.7978 - val_loss: 1.6718 - val_accuracy: 0.3600\n",
      "Epoch 101/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.4975 - accuracy: 0.8200 - val_loss: 1.7489 - val_accuracy: 0.3500\n",
      "Epoch 102/500\n",
      "900/900 [==============================] - 0s 331us/sample - loss: 0.4897 - accuracy: 0.8144 - val_loss: 1.7110 - val_accuracy: 0.3400\n",
      "Epoch 103/500\n",
      "900/900 [==============================] - 0s 345us/sample - loss: 0.5284 - accuracy: 0.8100 - val_loss: 1.7004 - val_accuracy: 0.3600\n",
      "Epoch 104/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.5250 - accuracy: 0.8044 - val_loss: 1.6848 - val_accuracy: 0.3900\n",
      "Epoch 105/500\n",
      "900/900 [==============================] - 0s 335us/sample - loss: 0.5001 - accuracy: 0.8111 - val_loss: 1.7347 - val_accuracy: 0.4000\n",
      "Epoch 106/500\n",
      "900/900 [==============================] - 0s 328us/sample - loss: 0.4661 - accuracy: 0.8278 - val_loss: 1.6687 - val_accuracy: 0.3800\n",
      "Epoch 107/500\n",
      "900/900 [==============================] - 0s 335us/sample - loss: 0.5158 - accuracy: 0.8056 - val_loss: 1.6820 - val_accuracy: 0.3700\n",
      "Epoch 108/500\n",
      "900/900 [==============================] - 0s 325us/sample - loss: 0.4582 - accuracy: 0.8256 - val_loss: 1.8097 - val_accuracy: 0.3100\n",
      "Epoch 109/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.5129 - accuracy: 0.8033 - val_loss: 1.7609 - val_accuracy: 0.3300\n",
      "Epoch 110/500\n",
      "900/900 [==============================] - 0s 332us/sample - loss: 0.4766 - accuracy: 0.8233 - val_loss: 1.7503 - val_accuracy: 0.3600\n",
      "Epoch 111/500\n",
      "900/900 [==============================] - 0s 328us/sample - loss: 0.4823 - accuracy: 0.8222 - val_loss: 1.6803 - val_accuracy: 0.3800\n",
      "Epoch 112/500\n",
      "900/900 [==============================] - 0s 334us/sample - loss: 0.4328 - accuracy: 0.8367 - val_loss: 1.7213 - val_accuracy: 0.3500\n",
      "Epoch 113/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.4652 - accuracy: 0.8256 - val_loss: 1.7331 - val_accuracy: 0.3800\n",
      "Epoch 114/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.4350 - accuracy: 0.8444 - val_loss: 1.6934 - val_accuracy: 0.4000\n",
      "Epoch 115/500\n",
      "900/900 [==============================] - 0s 335us/sample - loss: 0.4163 - accuracy: 0.8544 - val_loss: 1.7295 - val_accuracy: 0.3300\n",
      "Epoch 116/500\n",
      "900/900 [==============================] - 0s 328us/sample - loss: 0.4147 - accuracy: 0.8422 - val_loss: 1.7906 - val_accuracy: 0.3300\n",
      "Epoch 117/500\n",
      "900/900 [==============================] - 0s 327us/sample - loss: 0.4437 - accuracy: 0.8344 - val_loss: 1.7557 - val_accuracy: 0.3300\n",
      "Epoch 118/500\n",
      "900/900 [==============================] - 0s 371us/sample - loss: 0.3969 - accuracy: 0.8633 - val_loss: 1.7826 - val_accuracy: 0.3300\n",
      "Epoch 119/500\n",
      "900/900 [==============================] - 0s 336us/sample - loss: 0.4322 - accuracy: 0.8378 - val_loss: 1.7438 - val_accuracy: 0.3500\n",
      "Epoch 120/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.4188 - accuracy: 0.8389 - val_loss: 1.7877 - val_accuracy: 0.3400\n",
      "Epoch 121/500\n",
      "900/900 [==============================] - 0s 337us/sample - loss: 0.4107 - accuracy: 0.8467 - val_loss: 1.8600 - val_accuracy: 0.3400\n",
      "Epoch 122/500\n",
      "900/900 [==============================] - 0s 317us/sample - loss: 0.4109 - accuracy: 0.8644 - val_loss: 1.8644 - val_accuracy: 0.3300\n",
      "Epoch 123/500\n",
      "900/900 [==============================] - 0s 328us/sample - loss: 0.4110 - accuracy: 0.8544 - val_loss: 1.7900 - val_accuracy: 0.3500\n",
      "Epoch 124/500\n",
      "900/900 [==============================] - 0s 331us/sample - loss: 0.3985 - accuracy: 0.8544 - val_loss: 1.7978 - val_accuracy: 0.3400\n",
      "Epoch 125/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.4254 - accuracy: 0.8422 - val_loss: 1.8592 - val_accuracy: 0.3600\n",
      "Epoch 126/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.3852 - accuracy: 0.8600 - val_loss: 1.9105 - val_accuracy: 0.3600\n",
      "Epoch 127/500\n",
      "900/900 [==============================] - 0s 336us/sample - loss: 0.4049 - accuracy: 0.8478 - val_loss: 1.7330 - val_accuracy: 0.3900\n",
      "Epoch 128/500\n",
      "900/900 [==============================] - 0s 324us/sample - loss: 0.3713 - accuracy: 0.8589 - val_loss: 1.7714 - val_accuracy: 0.3900\n",
      "Epoch 129/500\n",
      "900/900 [==============================] - 0s 314us/sample - loss: 0.3748 - accuracy: 0.8678 - val_loss: 1.8662 - val_accuracy: 0.3600\n",
      "Epoch 130/500\n",
      "900/900 [==============================] - 0s 325us/sample - loss: 0.4145 - accuracy: 0.8378 - val_loss: 1.8907 - val_accuracy: 0.3800\n",
      "Epoch 131/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.3433 - accuracy: 0.8867 - val_loss: 1.8514 - val_accuracy: 0.3700\n",
      "Epoch 132/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.3758 - accuracy: 0.8611 - val_loss: 1.9775 - val_accuracy: 0.3600\n",
      "Epoch 133/500\n",
      "900/900 [==============================] - 0s 327us/sample - loss: 0.3528 - accuracy: 0.8744 - val_loss: 1.9306 - val_accuracy: 0.3700\n",
      "Epoch 134/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.3724 - accuracy: 0.8600 - val_loss: 1.9571 - val_accuracy: 0.3900\n",
      "Epoch 135/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.3641 - accuracy: 0.8700 - val_loss: 1.9859 - val_accuracy: 0.3700\n",
      "Epoch 136/500\n",
      "900/900 [==============================] - 0s 327us/sample - loss: 0.3474 - accuracy: 0.8778 - val_loss: 2.0226 - val_accuracy: 0.3600\n",
      "Epoch 137/500\n",
      "900/900 [==============================] - 0s 337us/sample - loss: 0.3325 - accuracy: 0.8822 - val_loss: 2.0019 - val_accuracy: 0.3600\n",
      "Epoch 138/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.3453 - accuracy: 0.8789 - val_loss: 2.0238 - val_accuracy: 0.3500\n",
      "Epoch 139/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.3386 - accuracy: 0.8822 - val_loss: 2.0563 - val_accuracy: 0.3700\n",
      "Epoch 140/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.3649 - accuracy: 0.8822 - val_loss: 2.0506 - val_accuracy: 0.3200\n",
      "Epoch 141/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.3580 - accuracy: 0.8744 - val_loss: 2.0383 - val_accuracy: 0.3100\n",
      "Epoch 142/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.3548 - accuracy: 0.8778 - val_loss: 2.0138 - val_accuracy: 0.3500\n",
      "Epoch 143/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.3738 - accuracy: 0.8556 - val_loss: 2.0688 - val_accuracy: 0.2900\n",
      "Epoch 144/500\n",
      "900/900 [==============================] - 0s 313us/sample - loss: 0.3296 - accuracy: 0.8778 - val_loss: 2.0116 - val_accuracy: 0.3200\n",
      "Epoch 145/500\n",
      "900/900 [==============================] - 0s 319us/sample - loss: 0.3610 - accuracy: 0.8656 - val_loss: 2.0386 - val_accuracy: 0.3300\n",
      "Epoch 146/500\n",
      "900/900 [==============================] - 0s 325us/sample - loss: 0.3569 - accuracy: 0.8700 - val_loss: 2.0109 - val_accuracy: 0.3700\n",
      "Epoch 147/500\n",
      "900/900 [==============================] - 0s 319us/sample - loss: 0.3095 - accuracy: 0.8989 - val_loss: 2.0363 - val_accuracy: 0.3500\n",
      "Epoch 148/500\n",
      "900/900 [==============================] - 0s 328us/sample - loss: 0.3066 - accuracy: 0.8856 - val_loss: 2.0463 - val_accuracy: 0.3500\n",
      "Epoch 149/500\n",
      "900/900 [==============================] - 0s 331us/sample - loss: 0.2976 - accuracy: 0.8933 - val_loss: 2.0181 - val_accuracy: 0.3400\n",
      "Epoch 150/500\n",
      "900/900 [==============================] - 0s 321us/sample - loss: 0.3576 - accuracy: 0.8722 - val_loss: 2.0218 - val_accuracy: 0.3600\n",
      "Epoch 151/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.3676 - accuracy: 0.8722 - val_loss: 2.0851 - val_accuracy: 0.3400\n",
      "Epoch 152/500\n",
      "900/900 [==============================] - 0s 317us/sample - loss: 0.3458 - accuracy: 0.8744 - val_loss: 2.0276 - val_accuracy: 0.3400\n",
      "Epoch 153/500\n",
      "900/900 [==============================] - 0s 416us/sample - loss: 0.3254 - accuracy: 0.8800 - val_loss: 1.9331 - val_accuracy: 0.3700\n",
      "Epoch 154/500\n",
      "900/900 [==============================] - 0s 475us/sample - loss: 0.3203 - accuracy: 0.8844 - val_loss: 1.9831 - val_accuracy: 0.3500\n",
      "Epoch 155/500\n",
      "900/900 [==============================] - 0s 478us/sample - loss: 0.2965 - accuracy: 0.8922 - val_loss: 1.9306 - val_accuracy: 0.3700\n",
      "Epoch 156/500\n",
      "900/900 [==============================] - 0s 337us/sample - loss: 0.3461 - accuracy: 0.8733 - val_loss: 1.9536 - val_accuracy: 0.3700\n",
      "Epoch 157/500\n",
      "900/900 [==============================] - 0s 356us/sample - loss: 0.3259 - accuracy: 0.8789 - val_loss: 1.9522 - val_accuracy: 0.3900\n",
      "Epoch 158/500\n",
      "900/900 [==============================] - 0s 331us/sample - loss: 0.3153 - accuracy: 0.8778 - val_loss: 1.9978 - val_accuracy: 0.3800\n",
      "Epoch 159/500\n",
      "900/900 [==============================] - 0s 377us/sample - loss: 0.3303 - accuracy: 0.8667 - val_loss: 2.0452 - val_accuracy: 0.3500\n",
      "Epoch 160/500\n",
      "900/900 [==============================] - 0s 402us/sample - loss: 0.3182 - accuracy: 0.8867 - val_loss: 2.0928 - val_accuracy: 0.3700\n",
      "Epoch 161/500\n",
      "900/900 [==============================] - 0s 309us/sample - loss: 0.3055 - accuracy: 0.8867 - val_loss: 2.0730 - val_accuracy: 0.3300\n",
      "Epoch 162/500\n",
      "900/900 [==============================] - 0s 346us/sample - loss: 0.3478 - accuracy: 0.8744 - val_loss: 1.9903 - val_accuracy: 0.3600\n",
      "Epoch 163/500\n",
      "900/900 [==============================] - 0s 417us/sample - loss: 0.3246 - accuracy: 0.8856 - val_loss: 1.9078 - val_accuracy: 0.3200\n",
      "Epoch 164/500\n",
      "900/900 [==============================] - 0s 447us/sample - loss: 0.3395 - accuracy: 0.8833 - val_loss: 1.8855 - val_accuracy: 0.3400\n",
      "Epoch 165/500\n",
      "900/900 [==============================] - 0s 424us/sample - loss: 0.3117 - accuracy: 0.8911 - val_loss: 1.9811 - val_accuracy: 0.3400\n",
      "Epoch 166/500\n",
      "900/900 [==============================] - 0s 310us/sample - loss: 0.3187 - accuracy: 0.8856 - val_loss: 2.0217 - val_accuracy: 0.3300\n",
      "Epoch 167/500\n",
      "900/900 [==============================] - 0s 367us/sample - loss: 0.3113 - accuracy: 0.8844 - val_loss: 1.9713 - val_accuracy: 0.3300\n",
      "Epoch 168/500\n",
      "900/900 [==============================] - 0s 385us/sample - loss: 0.3165 - accuracy: 0.8867 - val_loss: 1.9422 - val_accuracy: 0.3400\n",
      "Epoch 169/500\n",
      "900/900 [==============================] - 0s 450us/sample - loss: 0.2992 - accuracy: 0.8922 - val_loss: 2.0318 - val_accuracy: 0.3400\n",
      "Epoch 170/500\n",
      "900/900 [==============================] - 0s 423us/sample - loss: 0.3148 - accuracy: 0.8811 - val_loss: 2.0501 - val_accuracy: 0.3100\n",
      "Epoch 171/500\n",
      "900/900 [==============================] - 0s 417us/sample - loss: 0.2849 - accuracy: 0.9100 - val_loss: 2.1175 - val_accuracy: 0.3000\n",
      "Epoch 172/500\n",
      "900/900 [==============================] - 0s 491us/sample - loss: 0.2946 - accuracy: 0.8800 - val_loss: 2.0727 - val_accuracy: 0.3300\n",
      "Epoch 173/500\n",
      "900/900 [==============================] - 0s 454us/sample - loss: 0.2852 - accuracy: 0.8967 - val_loss: 2.0856 - val_accuracy: 0.3200\n",
      "Epoch 174/500\n",
      "900/900 [==============================] - 0s 481us/sample - loss: 0.2978 - accuracy: 0.9022 - val_loss: 2.1312 - val_accuracy: 0.3200\n",
      "Epoch 175/500\n",
      "900/900 [==============================] - 0s 512us/sample - loss: 0.2788 - accuracy: 0.9022 - val_loss: 2.1123 - val_accuracy: 0.3300\n",
      "Epoch 176/500\n",
      "900/900 [==============================] - 0s 495us/sample - loss: 0.2819 - accuracy: 0.9033 - val_loss: 1.9784 - val_accuracy: 0.3400\n",
      "Epoch 177/500\n",
      "900/900 [==============================] - 0s 488us/sample - loss: 0.3023 - accuracy: 0.8833 - val_loss: 1.9662 - val_accuracy: 0.3500\n",
      "Epoch 178/500\n",
      "900/900 [==============================] - 0s 424us/sample - loss: 0.2826 - accuracy: 0.9011 - val_loss: 2.0167 - val_accuracy: 0.3300\n",
      "Epoch 179/500\n",
      "900/900 [==============================] - 0s 482us/sample - loss: 0.2736 - accuracy: 0.9100 - val_loss: 2.0042 - val_accuracy: 0.3500\n",
      "Epoch 180/500\n",
      "900/900 [==============================] - 0s 453us/sample - loss: 0.2696 - accuracy: 0.9122 - val_loss: 2.0296 - val_accuracy: 0.3400\n",
      "Epoch 181/500\n",
      "900/900 [==============================] - 0s 473us/sample - loss: 0.2505 - accuracy: 0.9133 - val_loss: 2.1016 - val_accuracy: 0.3000\n",
      "Epoch 182/500\n",
      "900/900 [==============================] - 0s 308us/sample - loss: 0.2741 - accuracy: 0.9011 - val_loss: 2.0944 - val_accuracy: 0.3700\n",
      "Epoch 183/500\n",
      "900/900 [==============================] - 0s 315us/sample - loss: 0.2965 - accuracy: 0.8956 - val_loss: 2.0482 - val_accuracy: 0.3700\n",
      "Epoch 184/500\n",
      "900/900 [==============================] - 0s 319us/sample - loss: 0.2860 - accuracy: 0.9044 - val_loss: 2.0782 - val_accuracy: 0.3300\n",
      "Epoch 185/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.2897 - accuracy: 0.8844 - val_loss: 2.1483 - val_accuracy: 0.3400\n",
      "Epoch 186/500\n",
      "900/900 [==============================] - 0s 317us/sample - loss: 0.2856 - accuracy: 0.9044 - val_loss: 1.9926 - val_accuracy: 0.3900\n",
      "Epoch 187/500\n",
      "900/900 [==============================] - 0s 395us/sample - loss: 0.2865 - accuracy: 0.8989 - val_loss: 1.9787 - val_accuracy: 0.3800\n",
      "Epoch 188/500\n",
      "900/900 [==============================] - 0s 324us/sample - loss: 0.2579 - accuracy: 0.9100 - val_loss: 2.0905 - val_accuracy: 0.3100\n",
      "Epoch 189/500\n",
      "900/900 [==============================] - 0s 312us/sample - loss: 0.3004 - accuracy: 0.8978 - val_loss: 2.1703 - val_accuracy: 0.3500\n",
      "Epoch 190/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.3360 - accuracy: 0.8789 - val_loss: 2.1978 - val_accuracy: 0.3200\n",
      "Epoch 191/500\n",
      "900/900 [==============================] - 0s 411us/sample - loss: 0.3129 - accuracy: 0.8944 - val_loss: 2.2005 - val_accuracy: 0.3500\n",
      "Epoch 192/500\n",
      "900/900 [==============================] - 0s 310us/sample - loss: 0.3103 - accuracy: 0.8889 - val_loss: 2.2029 - val_accuracy: 0.3400\n",
      "Epoch 193/500\n",
      "900/900 [==============================] - 0s 407us/sample - loss: 0.2687 - accuracy: 0.9033 - val_loss: 2.2451 - val_accuracy: 0.3200\n",
      "Epoch 194/500\n",
      "900/900 [==============================] - 0s 328us/sample - loss: 0.2324 - accuracy: 0.9011 - val_loss: 2.2861 - val_accuracy: 0.3600\n",
      "Epoch 195/500\n",
      "900/900 [==============================] - 0s 324us/sample - loss: 0.2585 - accuracy: 0.9056 - val_loss: 2.3436 - val_accuracy: 0.3400\n",
      "Epoch 196/500\n",
      "900/900 [==============================] - 0s 533us/sample - loss: 0.2766 - accuracy: 0.8989 - val_loss: 2.2490 - val_accuracy: 0.3300\n",
      "Epoch 197/500\n",
      "900/900 [==============================] - 0s 375us/sample - loss: 0.2915 - accuracy: 0.8967 - val_loss: 2.1892 - val_accuracy: 0.3600\n",
      "Epoch 198/500\n",
      "900/900 [==============================] - 0s 454us/sample - loss: 0.2584 - accuracy: 0.9067 - val_loss: 2.1795 - val_accuracy: 0.3300\n",
      "Epoch 199/500\n",
      "900/900 [==============================] - 0s 475us/sample - loss: 0.2784 - accuracy: 0.8967 - val_loss: 2.2508 - val_accuracy: 0.3300\n",
      "Epoch 200/500\n",
      "900/900 [==============================] - 0s 521us/sample - loss: 0.2502 - accuracy: 0.9100 - val_loss: 2.2189 - val_accuracy: 0.3600\n",
      "Epoch 201/500\n",
      "900/900 [==============================] - 0s 518us/sample - loss: 0.2741 - accuracy: 0.8978 - val_loss: 2.2068 - val_accuracy: 0.3500\n",
      "Epoch 202/500\n",
      "900/900 [==============================] - 0s 485us/sample - loss: 0.2516 - accuracy: 0.9089 - val_loss: 2.2327 - val_accuracy: 0.3300\n",
      "Epoch 203/500\n",
      "900/900 [==============================] - 0s 461us/sample - loss: 0.2526 - accuracy: 0.9067 - val_loss: 2.2423 - val_accuracy: 0.3400\n",
      "Epoch 204/500\n",
      "900/900 [==============================] - 0s 325us/sample - loss: 0.2614 - accuracy: 0.9044 - val_loss: 2.2840 - val_accuracy: 0.3300\n",
      "Epoch 205/500\n",
      "900/900 [==============================] - 0s 338us/sample - loss: 0.2584 - accuracy: 0.9022 - val_loss: 2.2481 - val_accuracy: 0.3300\n",
      "Epoch 206/500\n",
      "900/900 [==============================] - 0s 373us/sample - loss: 0.2652 - accuracy: 0.9056 - val_loss: 2.1751 - val_accuracy: 0.3800\n",
      "Epoch 207/500\n",
      "900/900 [==============================] - 0s 473us/sample - loss: 0.2544 - accuracy: 0.9056 - val_loss: 2.1089 - val_accuracy: 0.3400\n",
      "Epoch 208/500\n",
      "900/900 [==============================] - 0s 480us/sample - loss: 0.2623 - accuracy: 0.9000 - val_loss: 2.1310 - val_accuracy: 0.3400\n",
      "Epoch 209/500\n",
      "900/900 [==============================] - 0s 297us/sample - loss: 0.2816 - accuracy: 0.9089 - val_loss: 2.1614 - val_accuracy: 0.3400\n",
      "Epoch 210/500\n",
      "900/900 [==============================] - 0s 372us/sample - loss: 0.2568 - accuracy: 0.9100 - val_loss: 2.1055 - val_accuracy: 0.3700\n",
      "Epoch 211/500\n",
      "900/900 [==============================] - 0s 432us/sample - loss: 0.2602 - accuracy: 0.9067 - val_loss: 2.1822 - val_accuracy: 0.3600\n",
      "Epoch 212/500\n",
      "900/900 [==============================] - 0s 472us/sample - loss: 0.2757 - accuracy: 0.8989 - val_loss: 2.2636 - val_accuracy: 0.3600\n",
      "Epoch 213/500\n",
      "900/900 [==============================] - 0s 445us/sample - loss: 0.2743 - accuracy: 0.9056 - val_loss: 2.3949 - val_accuracy: 0.3600\n",
      "Epoch 214/500\n",
      "900/900 [==============================] - 0s 464us/sample - loss: 0.2374 - accuracy: 0.9067 - val_loss: 2.4043 - val_accuracy: 0.3600\n",
      "Epoch 215/500\n",
      "900/900 [==============================] - 0s 474us/sample - loss: 0.3064 - accuracy: 0.8911 - val_loss: 2.3507 - val_accuracy: 0.3500\n",
      "Epoch 216/500\n",
      "900/900 [==============================] - 0s 449us/sample - loss: 0.2700 - accuracy: 0.9044 - val_loss: 2.3515 - val_accuracy: 0.3500\n",
      "Epoch 217/500\n",
      "900/900 [==============================] - 0s 308us/sample - loss: 0.2547 - accuracy: 0.9044 - val_loss: 2.3391 - val_accuracy: 0.3600\n",
      "Epoch 218/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.2244 - accuracy: 0.9189 - val_loss: 2.3080 - val_accuracy: 0.3600\n",
      "Epoch 219/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.2464 - accuracy: 0.9044 - val_loss: 2.3694 - val_accuracy: 0.3500\n",
      "Epoch 220/500\n",
      "900/900 [==============================] - 0s 310us/sample - loss: 0.2443 - accuracy: 0.9156 - val_loss: 2.3899 - val_accuracy: 0.3800\n",
      "Epoch 221/500\n",
      "900/900 [==============================] - 0s 310us/sample - loss: 0.2561 - accuracy: 0.9011 - val_loss: 2.4036 - val_accuracy: 0.3600\n",
      "Epoch 222/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.2273 - accuracy: 0.9122 - val_loss: 2.3833 - val_accuracy: 0.3600\n",
      "Epoch 223/500\n",
      "900/900 [==============================] - 0s 316us/sample - loss: 0.2364 - accuracy: 0.9200 - val_loss: 2.3251 - val_accuracy: 0.3700\n",
      "Epoch 224/500\n",
      "900/900 [==============================] - 0s 309us/sample - loss: 0.2378 - accuracy: 0.9167 - val_loss: 2.3805 - val_accuracy: 0.3600\n",
      "Epoch 225/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.2501 - accuracy: 0.9111 - val_loss: 2.4079 - val_accuracy: 0.3700\n",
      "Epoch 226/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2717 - accuracy: 0.9078 - val_loss: 2.5176 - val_accuracy: 0.3400\n",
      "Epoch 227/500\n",
      "900/900 [==============================] - 0s 381us/sample - loss: 0.2533 - accuracy: 0.9033 - val_loss: 2.5188 - val_accuracy: 0.3400\n",
      "Epoch 228/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.2671 - accuracy: 0.9011 - val_loss: 2.4588 - val_accuracy: 0.3100\n",
      "Epoch 229/500\n",
      "900/900 [==============================] - 0s 308us/sample - loss: 0.2617 - accuracy: 0.9111 - val_loss: 2.3535 - val_accuracy: 0.3000\n",
      "Epoch 230/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.2742 - accuracy: 0.9056 - val_loss: 2.3197 - val_accuracy: 0.3100\n",
      "Epoch 231/500\n",
      "900/900 [==============================] - 0s 314us/sample - loss: 0.2299 - accuracy: 0.9211 - val_loss: 2.2767 - val_accuracy: 0.3300\n",
      "Epoch 232/500\n",
      "900/900 [==============================] - 0s 304us/sample - loss: 0.2822 - accuracy: 0.9067 - val_loss: 2.3543 - val_accuracy: 0.3700\n",
      "Epoch 233/500\n",
      "900/900 [==============================] - 0s 304us/sample - loss: 0.2489 - accuracy: 0.9133 - val_loss: 2.3995 - val_accuracy: 0.3700\n",
      "Epoch 234/500\n",
      "900/900 [==============================] - 0s 308us/sample - loss: 0.2458 - accuracy: 0.9267 - val_loss: 2.3419 - val_accuracy: 0.3300\n",
      "Epoch 235/500\n",
      "900/900 [==============================] - 0s 305us/sample - loss: 0.2637 - accuracy: 0.9156 - val_loss: 2.3374 - val_accuracy: 0.3700\n",
      "Epoch 236/500\n",
      "900/900 [==============================] - 0s 306us/sample - loss: 0.2171 - accuracy: 0.9222 - val_loss: 2.3564 - val_accuracy: 0.3600\n",
      "Epoch 237/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.2428 - accuracy: 0.9144 - val_loss: 2.4815 - val_accuracy: 0.3200\n",
      "Epoch 238/500\n",
      "900/900 [==============================] - 0s 288us/sample - loss: 0.2056 - accuracy: 0.9344 - val_loss: 2.4556 - val_accuracy: 0.3200\n",
      "Epoch 239/500\n",
      "900/900 [==============================] - 0s 288us/sample - loss: 0.2226 - accuracy: 0.9233 - val_loss: 2.3855 - val_accuracy: 0.3500\n",
      "Epoch 240/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.2318 - accuracy: 0.9156 - val_loss: 2.4866 - val_accuracy: 0.3600\n",
      "Epoch 241/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2381 - accuracy: 0.9122 - val_loss: 2.4335 - val_accuracy: 0.3600\n",
      "Epoch 242/500\n",
      "900/900 [==============================] - 0s 304us/sample - loss: 0.2286 - accuracy: 0.9189 - val_loss: 2.3984 - val_accuracy: 0.3800\n",
      "Epoch 243/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.2435 - accuracy: 0.9144 - val_loss: 2.3943 - val_accuracy: 0.3600\n",
      "Epoch 244/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.2260 - accuracy: 0.9156 - val_loss: 2.3809 - val_accuracy: 0.3600\n",
      "Epoch 245/500\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.92 - 0s 314us/sample - loss: 0.2153 - accuracy: 0.9289 - val_loss: 2.4040 - val_accuracy: 0.3700\n",
      "Epoch 246/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2337 - accuracy: 0.9167 - val_loss: 2.3694 - val_accuracy: 0.3800\n",
      "Epoch 247/500\n",
      "900/900 [==============================] - 0s 322us/sample - loss: 0.2163 - accuracy: 0.9300 - val_loss: 2.3951 - val_accuracy: 0.3500\n",
      "Epoch 248/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2157 - accuracy: 0.9322 - val_loss: 2.3849 - val_accuracy: 0.3400\n",
      "Epoch 249/500\n",
      "900/900 [==============================] - 0s 291us/sample - loss: 0.2345 - accuracy: 0.9133 - val_loss: 2.3617 - val_accuracy: 0.3500\n",
      "Epoch 250/500\n",
      "900/900 [==============================] - 0s 297us/sample - loss: 0.2414 - accuracy: 0.9244 - val_loss: 2.2762 - val_accuracy: 0.3700\n",
      "Epoch 251/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.2132 - accuracy: 0.9256 - val_loss: 2.2654 - val_accuracy: 0.3600\n",
      "Epoch 252/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2656 - accuracy: 0.9022 - val_loss: 2.2897 - val_accuracy: 0.3500\n",
      "Epoch 253/500\n",
      "900/900 [==============================] - 0s 298us/sample - loss: 0.2246 - accuracy: 0.9189 - val_loss: 2.2626 - val_accuracy: 0.4000\n",
      "Epoch 254/500\n",
      "900/900 [==============================] - 0s 297us/sample - loss: 0.2366 - accuracy: 0.9156 - val_loss: 2.3550 - val_accuracy: 0.3800\n",
      "Epoch 255/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.2237 - accuracy: 0.9178 - val_loss: 2.4544 - val_accuracy: 0.3700\n",
      "Epoch 256/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.2305 - accuracy: 0.9189 - val_loss: 2.4817 - val_accuracy: 0.4000\n",
      "Epoch 257/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2389 - accuracy: 0.9111 - val_loss: 2.5288 - val_accuracy: 0.3800\n",
      "Epoch 258/500\n",
      "900/900 [==============================] - 0s 311us/sample - loss: 0.2343 - accuracy: 0.9133 - val_loss: 2.5402 - val_accuracy: 0.3700\n",
      "Epoch 259/500\n",
      "900/900 [==============================] - 0s 305us/sample - loss: 0.2233 - accuracy: 0.9167 - val_loss: 2.5073 - val_accuracy: 0.3700\n",
      "Epoch 260/500\n",
      "900/900 [==============================] - 0s 312us/sample - loss: 0.2234 - accuracy: 0.9233 - val_loss: 2.4570 - val_accuracy: 0.3700\n",
      "Epoch 261/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.2394 - accuracy: 0.9233 - val_loss: 2.4286 - val_accuracy: 0.3900\n",
      "Epoch 262/500\n",
      "900/900 [==============================] - 0s 305us/sample - loss: 0.2102 - accuracy: 0.9311 - val_loss: 2.3853 - val_accuracy: 0.3900\n",
      "Epoch 263/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1974 - accuracy: 0.9300 - val_loss: 2.3765 - val_accuracy: 0.3900\n",
      "Epoch 264/500\n",
      "900/900 [==============================] - 0s 298us/sample - loss: 0.2148 - accuracy: 0.9244 - val_loss: 2.4192 - val_accuracy: 0.3500\n",
      "Epoch 265/500\n",
      "900/900 [==============================] - 0s 286us/sample - loss: 0.2007 - accuracy: 0.9356 - val_loss: 2.3775 - val_accuracy: 0.3400\n",
      "Epoch 266/500\n",
      "900/900 [==============================] - 0s 305us/sample - loss: 0.2385 - accuracy: 0.9144 - val_loss: 2.3426 - val_accuracy: 0.3400\n",
      "Epoch 267/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.2409 - accuracy: 0.9067 - val_loss: 2.4825 - val_accuracy: 0.3300\n",
      "Epoch 268/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2335 - accuracy: 0.9200 - val_loss: 2.5195 - val_accuracy: 0.3300\n",
      "Epoch 269/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.2051 - accuracy: 0.9233 - val_loss: 2.5592 - val_accuracy: 0.3400\n",
      "Epoch 270/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.2192 - accuracy: 0.9100 - val_loss: 2.5949 - val_accuracy: 0.3400\n",
      "Epoch 271/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2481 - accuracy: 0.9156 - val_loss: 2.5811 - val_accuracy: 0.3300\n",
      "Epoch 272/500\n",
      "900/900 [==============================] - 0s 287us/sample - loss: 0.1973 - accuracy: 0.9244 - val_loss: 2.5963 - val_accuracy: 0.3600\n",
      "Epoch 273/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2495 - accuracy: 0.9156 - val_loss: 2.5829 - val_accuracy: 0.3600\n",
      "Epoch 274/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.1939 - accuracy: 0.9356 - val_loss: 2.5883 - val_accuracy: 0.3400\n",
      "Epoch 275/500\n",
      "900/900 [==============================] - 0s 291us/sample - loss: 0.2024 - accuracy: 0.9289 - val_loss: 2.6411 - val_accuracy: 0.3200\n",
      "Epoch 276/500\n",
      "900/900 [==============================] - 0s 299us/sample - loss: 0.2127 - accuracy: 0.9278 - val_loss: 2.6502 - val_accuracy: 0.3300\n",
      "Epoch 277/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.2544 - accuracy: 0.9156 - val_loss: 2.6364 - val_accuracy: 0.3300\n",
      "Epoch 278/500\n",
      "900/900 [==============================] - 0s 308us/sample - loss: 0.2019 - accuracy: 0.9289 - val_loss: 2.6064 - val_accuracy: 0.3300\n",
      "Epoch 279/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.1977 - accuracy: 0.9333 - val_loss: 2.5314 - val_accuracy: 0.3300\n",
      "Epoch 280/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1950 - accuracy: 0.9389 - val_loss: 2.5269 - val_accuracy: 0.3200\n",
      "Epoch 281/500\n",
      "900/900 [==============================] - 0s 297us/sample - loss: 0.2032 - accuracy: 0.9300 - val_loss: 2.5928 - val_accuracy: 0.3100\n",
      "Epoch 282/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.2099 - accuracy: 0.9278 - val_loss: 2.6711 - val_accuracy: 0.3100\n",
      "Epoch 283/500\n",
      "900/900 [==============================] - 0s 309us/sample - loss: 0.1891 - accuracy: 0.9378 - val_loss: 2.7115 - val_accuracy: 0.3200\n",
      "Epoch 284/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.2159 - accuracy: 0.9211 - val_loss: 2.5538 - val_accuracy: 0.3200\n",
      "Epoch 285/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.2072 - accuracy: 0.9289 - val_loss: 2.6009 - val_accuracy: 0.3300\n",
      "Epoch 286/500\n",
      "900/900 [==============================] - 0s 299us/sample - loss: 0.1809 - accuracy: 0.9389 - val_loss: 2.6760 - val_accuracy: 0.3300\n",
      "Epoch 287/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.2364 - accuracy: 0.9189 - val_loss: 2.6346 - val_accuracy: 0.3700\n",
      "Epoch 288/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.2324 - accuracy: 0.9211 - val_loss: 2.6243 - val_accuracy: 0.3500\n",
      "Epoch 289/500\n",
      "900/900 [==============================] - 0s 298us/sample - loss: 0.2297 - accuracy: 0.9211 - val_loss: 2.6017 - val_accuracy: 0.3300\n",
      "Epoch 290/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.2091 - accuracy: 0.9300 - val_loss: 2.6644 - val_accuracy: 0.3200\n",
      "Epoch 291/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1948 - accuracy: 0.9322 - val_loss: 2.7593 - val_accuracy: 0.3200\n",
      "Epoch 292/500\n",
      "900/900 [==============================] - 0s 291us/sample - loss: 0.2143 - accuracy: 0.9300 - val_loss: 2.7387 - val_accuracy: 0.3300\n",
      "Epoch 293/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.2046 - accuracy: 0.9289 - val_loss: 2.6995 - val_accuracy: 0.3600\n",
      "Epoch 294/500\n",
      "900/900 [==============================] - 0s 304us/sample - loss: 0.1785 - accuracy: 0.9344 - val_loss: 2.6582 - val_accuracy: 0.3700\n",
      "Epoch 295/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.2270 - accuracy: 0.9178 - val_loss: 2.7168 - val_accuracy: 0.3400\n",
      "Epoch 296/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.2206 - accuracy: 0.9211 - val_loss: 2.7544 - val_accuracy: 0.3500\n",
      "Epoch 297/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.2084 - accuracy: 0.9289 - val_loss: 2.6621 - val_accuracy: 0.3700\n",
      "Epoch 298/500\n",
      "900/900 [==============================] - 0s 289us/sample - loss: 0.1994 - accuracy: 0.9367 - val_loss: 2.6799 - val_accuracy: 0.3300\n",
      "Epoch 299/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1912 - accuracy: 0.9289 - val_loss: 2.5998 - val_accuracy: 0.3400\n",
      "Epoch 300/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.1870 - accuracy: 0.9344 - val_loss: 2.5519 - val_accuracy: 0.3300\n",
      "Epoch 301/500\n",
      "900/900 [==============================] - 0s 293us/sample - loss: 0.1927 - accuracy: 0.9344 - val_loss: 2.5783 - val_accuracy: 0.3600\n",
      "Epoch 302/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1894 - accuracy: 0.9333 - val_loss: 2.6567 - val_accuracy: 0.3300\n",
      "Epoch 303/500\n",
      "900/900 [==============================] - 0s 293us/sample - loss: 0.2283 - accuracy: 0.9211 - val_loss: 2.5560 - val_accuracy: 0.3300\n",
      "Epoch 304/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1974 - accuracy: 0.9311 - val_loss: 2.5619 - val_accuracy: 0.3600\n",
      "Epoch 305/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.1806 - accuracy: 0.9400 - val_loss: 2.5534 - val_accuracy: 0.3200\n",
      "Epoch 306/500\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.93 - 0s 295us/sample - loss: 0.1947 - accuracy: 0.9344 - val_loss: 2.6556 - val_accuracy: 0.3000\n",
      "Epoch 307/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.2402 - accuracy: 0.9167 - val_loss: 2.6898 - val_accuracy: 0.3200\n",
      "Epoch 308/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1754 - accuracy: 0.9422 - val_loss: 2.6134 - val_accuracy: 0.3400\n",
      "Epoch 309/500\n",
      "900/900 [==============================] - 0s 306us/sample - loss: 0.2088 - accuracy: 0.9267 - val_loss: 2.5592 - val_accuracy: 0.3400\n",
      "Epoch 310/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1818 - accuracy: 0.9367 - val_loss: 2.5580 - val_accuracy: 0.3300\n",
      "Epoch 311/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1665 - accuracy: 0.9422 - val_loss: 2.5362 - val_accuracy: 0.3300\n",
      "Epoch 312/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.2076 - accuracy: 0.9244 - val_loss: 2.5223 - val_accuracy: 0.3700\n",
      "Epoch 313/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.1992 - accuracy: 0.9300 - val_loss: 2.6065 - val_accuracy: 0.3400\n",
      "Epoch 314/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.2109 - accuracy: 0.9278 - val_loss: 2.5805 - val_accuracy: 0.3300\n",
      "Epoch 315/500\n",
      "900/900 [==============================] - 0s 304us/sample - loss: 0.1767 - accuracy: 0.9356 - val_loss: 2.6015 - val_accuracy: 0.3500\n",
      "Epoch 316/500\n",
      "900/900 [==============================] - 0s 291us/sample - loss: 0.2236 - accuracy: 0.9267 - val_loss: 2.7020 - val_accuracy: 0.3500\n",
      "Epoch 317/500\n",
      "900/900 [==============================] - 0s 299us/sample - loss: 0.2214 - accuracy: 0.9233 - val_loss: 2.7182 - val_accuracy: 0.3300\n",
      "Epoch 318/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1916 - accuracy: 0.9422 - val_loss: 2.7625 - val_accuracy: 0.3400\n",
      "Epoch 319/500\n",
      "900/900 [==============================] - 0s 293us/sample - loss: 0.1897 - accuracy: 0.9367 - val_loss: 2.7294 - val_accuracy: 0.3300\n",
      "Epoch 320/500\n",
      "900/900 [==============================] - 0s 297us/sample - loss: 0.2115 - accuracy: 0.9333 - val_loss: 2.6944 - val_accuracy: 0.3700\n",
      "Epoch 321/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.2077 - accuracy: 0.9244 - val_loss: 2.6394 - val_accuracy: 0.3500\n",
      "Epoch 322/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.1920 - accuracy: 0.9333 - val_loss: 2.6439 - val_accuracy: 0.3400\n",
      "Epoch 323/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.2055 - accuracy: 0.9300 - val_loss: 2.6202 - val_accuracy: 0.3700\n",
      "Epoch 324/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.1984 - accuracy: 0.9322 - val_loss: 2.5939 - val_accuracy: 0.3600\n",
      "Epoch 325/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.1973 - accuracy: 0.9256 - val_loss: 2.6807 - val_accuracy: 0.3300\n",
      "Epoch 326/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.2030 - accuracy: 0.9233 - val_loss: 2.6503 - val_accuracy: 0.3300\n",
      "Epoch 327/500\n",
      "900/900 [==============================] - 0s 291us/sample - loss: 0.1791 - accuracy: 0.9400 - val_loss: 2.6923 - val_accuracy: 0.3500\n",
      "Epoch 328/500\n",
      "900/900 [==============================] - 0s 284us/sample - loss: 0.1766 - accuracy: 0.9456 - val_loss: 2.7031 - val_accuracy: 0.3500\n",
      "Epoch 329/500\n",
      "900/900 [==============================] - 0s 298us/sample - loss: 0.1833 - accuracy: 0.9356 - val_loss: 2.7035 - val_accuracy: 0.3400\n",
      "Epoch 330/500\n",
      "900/900 [==============================] - 0s 293us/sample - loss: 0.1942 - accuracy: 0.9411 - val_loss: 2.7449 - val_accuracy: 0.3400\n",
      "Epoch 331/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1752 - accuracy: 0.9344 - val_loss: 2.8596 - val_accuracy: 0.3400\n",
      "Epoch 332/500\n",
      "900/900 [==============================] - 0s 286us/sample - loss: 0.1933 - accuracy: 0.9278 - val_loss: 2.7263 - val_accuracy: 0.3600\n",
      "Epoch 333/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.2159 - accuracy: 0.9222 - val_loss: 2.6393 - val_accuracy: 0.3600\n",
      "Epoch 334/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.1760 - accuracy: 0.9411 - val_loss: 2.6622 - val_accuracy: 0.3800\n",
      "Epoch 335/500\n",
      "900/900 [==============================] - 0s 283us/sample - loss: 0.1773 - accuracy: 0.9478 - val_loss: 2.6529 - val_accuracy: 0.3600\n",
      "Epoch 336/500\n",
      "900/900 [==============================] - 0s 291us/sample - loss: 0.1811 - accuracy: 0.9378 - val_loss: 2.6391 - val_accuracy: 0.3400\n",
      "Epoch 337/500\n",
      "900/900 [==============================] - 0s 286us/sample - loss: 0.2038 - accuracy: 0.9300 - val_loss: 2.6067 - val_accuracy: 0.3300\n",
      "Epoch 338/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.1950 - accuracy: 0.9378 - val_loss: 2.6580 - val_accuracy: 0.3500\n",
      "Epoch 339/500\n",
      "900/900 [==============================] - 0s 289us/sample - loss: 0.2197 - accuracy: 0.9167 - val_loss: 2.6133 - val_accuracy: 0.3700\n",
      "Epoch 340/500\n",
      "900/900 [==============================] - 0s 293us/sample - loss: 0.1904 - accuracy: 0.9356 - val_loss: 2.6493 - val_accuracy: 0.3500\n",
      "Epoch 341/500\n",
      "900/900 [==============================] - 0s 297us/sample - loss: 0.2140 - accuracy: 0.9244 - val_loss: 2.6395 - val_accuracy: 0.3800\n",
      "Epoch 342/500\n",
      "900/900 [==============================] - 0s 326us/sample - loss: 0.1877 - accuracy: 0.9278 - val_loss: 2.6370 - val_accuracy: 0.3600\n",
      "Epoch 343/500\n",
      "900/900 [==============================] - 0s 349us/sample - loss: 0.1836 - accuracy: 0.9344 - val_loss: 2.6650 - val_accuracy: 0.3900\n",
      "Epoch 344/500\n",
      "900/900 [==============================] - 0s 407us/sample - loss: 0.1938 - accuracy: 0.9267 - val_loss: 2.7014 - val_accuracy: 0.3700\n",
      "Epoch 345/500\n",
      "900/900 [==============================] - 0s 344us/sample - loss: 0.2190 - accuracy: 0.9222 - val_loss: 2.6997 - val_accuracy: 0.4000\n",
      "Epoch 346/500\n",
      "900/900 [==============================] - 0s 434us/sample - loss: 0.1608 - accuracy: 0.9467 - val_loss: 2.7740 - val_accuracy: 0.3700\n",
      "Epoch 347/500\n",
      "900/900 [==============================] - 0s 429us/sample - loss: 0.1769 - accuracy: 0.9422 - val_loss: 2.8034 - val_accuracy: 0.3600\n",
      "Epoch 348/500\n",
      "900/900 [==============================] - 0s 314us/sample - loss: 0.1927 - accuracy: 0.9356 - val_loss: 2.7683 - val_accuracy: 0.3600\n",
      "Epoch 349/500\n",
      "900/900 [==============================] - 0s 324us/sample - loss: 0.2049 - accuracy: 0.9367 - val_loss: 2.8199 - val_accuracy: 0.3800\n",
      "Epoch 350/500\n",
      "900/900 [==============================] - 0s 367us/sample - loss: 0.1745 - accuracy: 0.9444 - val_loss: 2.8388 - val_accuracy: 0.3800\n",
      "Epoch 351/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.2120 - accuracy: 0.9167 - val_loss: 2.7838 - val_accuracy: 0.4100\n",
      "Epoch 352/500\n",
      "900/900 [==============================] - 0s 386us/sample - loss: 0.1772 - accuracy: 0.9456 - val_loss: 2.7151 - val_accuracy: 0.4200\n",
      "Epoch 353/500\n",
      "900/900 [==============================] - 0s 324us/sample - loss: 0.1679 - accuracy: 0.9322 - val_loss: 2.7147 - val_accuracy: 0.4100\n",
      "Epoch 354/500\n",
      "900/900 [==============================] - 0s 330us/sample - loss: 0.1738 - accuracy: 0.9389 - val_loss: 2.6931 - val_accuracy: 0.4100\n",
      "Epoch 355/500\n",
      "900/900 [==============================] - 0s 293us/sample - loss: 0.1841 - accuracy: 0.9411 - val_loss: 2.6611 - val_accuracy: 0.4200\n",
      "Epoch 356/500\n",
      "900/900 [==============================] - 0s 455us/sample - loss: 0.1839 - accuracy: 0.9367 - val_loss: 2.6728 - val_accuracy: 0.4200\n",
      "Epoch 357/500\n",
      "900/900 [==============================] - 0s 494us/sample - loss: 0.1739 - accuracy: 0.9400 - val_loss: 2.6270 - val_accuracy: 0.4300\n",
      "Epoch 358/500\n",
      "900/900 [==============================] - 1s 583us/sample - loss: 0.1732 - accuracy: 0.9289 - val_loss: 2.6941 - val_accuracy: 0.4000\n",
      "Epoch 359/500\n",
      "900/900 [==============================] - 0s 550us/sample - loss: 0.2031 - accuracy: 0.9333 - val_loss: 2.7812 - val_accuracy: 0.4000\n",
      "Epoch 360/500\n",
      "900/900 [==============================] - 1s 562us/sample - loss: 0.1702 - accuracy: 0.9422 - val_loss: 2.7475 - val_accuracy: 0.4300\n",
      "Epoch 361/500\n",
      "900/900 [==============================] - 0s 455us/sample - loss: 0.2002 - accuracy: 0.9311 - val_loss: 2.7480 - val_accuracy: 0.4000\n",
      "Epoch 362/500\n",
      "900/900 [==============================] - 0s 427us/sample - loss: 0.1884 - accuracy: 0.9322 - val_loss: 2.7461 - val_accuracy: 0.4100\n",
      "Epoch 363/500\n",
      "900/900 [==============================] - 0s 437us/sample - loss: 0.1731 - accuracy: 0.9433 - val_loss: 2.7260 - val_accuracy: 0.4000\n",
      "Epoch 364/500\n",
      "900/900 [==============================] - 0s 488us/sample - loss: 0.1713 - accuracy: 0.9433 - val_loss: 2.7933 - val_accuracy: 0.4000\n",
      "Epoch 365/500\n",
      "900/900 [==============================] - 0s 519us/sample - loss: 0.1715 - accuracy: 0.9367 - val_loss: 2.8336 - val_accuracy: 0.4000\n",
      "Epoch 366/500\n",
      "900/900 [==============================] - 0s 483us/sample - loss: 0.1757 - accuracy: 0.9389 - val_loss: 2.8677 - val_accuracy: 0.3900\n",
      "Epoch 367/500\n",
      "900/900 [==============================] - 0s 390us/sample - loss: 0.1696 - accuracy: 0.9433 - val_loss: 2.8315 - val_accuracy: 0.3800\n",
      "Epoch 368/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1875 - accuracy: 0.9322 - val_loss: 2.8591 - val_accuracy: 0.3800\n",
      "Epoch 369/500\n",
      "900/900 [==============================] - 0s 411us/sample - loss: 0.1627 - accuracy: 0.9444 - val_loss: 2.8223 - val_accuracy: 0.3900\n",
      "Epoch 370/500\n",
      "900/900 [==============================] - 0s 491us/sample - loss: 0.1719 - accuracy: 0.9322 - val_loss: 2.8128 - val_accuracy: 0.3900\n",
      "Epoch 371/500\n",
      "900/900 [==============================] - 0s 462us/sample - loss: 0.1880 - accuracy: 0.9333 - val_loss: 2.7677 - val_accuracy: 0.4000\n",
      "Epoch 372/500\n",
      "900/900 [==============================] - 0s 295us/sample - loss: 0.1931 - accuracy: 0.9322 - val_loss: 2.8243 - val_accuracy: 0.4000\n",
      "Epoch 373/500\n",
      "900/900 [==============================] - 0s 311us/sample - loss: 0.1609 - accuracy: 0.9467 - val_loss: 2.8769 - val_accuracy: 0.3800\n",
      "Epoch 374/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.1636 - accuracy: 0.9422 - val_loss: 2.9419 - val_accuracy: 0.4000\n",
      "Epoch 375/500\n",
      "900/900 [==============================] - 0s 504us/sample - loss: 0.2097 - accuracy: 0.9311 - val_loss: 2.8830 - val_accuracy: 0.4000\n",
      "Epoch 376/500\n",
      "900/900 [==============================] - 0s 549us/sample - loss: 0.1743 - accuracy: 0.9489 - val_loss: 2.7443 - val_accuracy: 0.3800\n",
      "Epoch 377/500\n",
      "900/900 [==============================] - 0s 413us/sample - loss: 0.1995 - accuracy: 0.9300 - val_loss: 2.7427 - val_accuracy: 0.3900\n",
      "Epoch 378/500\n",
      "900/900 [==============================] - 0s 516us/sample - loss: 0.2021 - accuracy: 0.9344 - val_loss: 2.7891 - val_accuracy: 0.3900\n",
      "Epoch 379/500\n",
      "900/900 [==============================] - 0s 450us/sample - loss: 0.1669 - accuracy: 0.9378 - val_loss: 2.7545 - val_accuracy: 0.3900\n",
      "Epoch 380/500\n",
      "900/900 [==============================] - 0s 483us/sample - loss: 0.1713 - accuracy: 0.9411 - val_loss: 2.7689 - val_accuracy: 0.3700\n",
      "Epoch 381/500\n",
      "900/900 [==============================] - 0s 469us/sample - loss: 0.1770 - accuracy: 0.9389 - val_loss: 2.8271 - val_accuracy: 0.3600\n",
      "Epoch 382/500\n",
      "900/900 [==============================] - 0s 443us/sample - loss: 0.1651 - accuracy: 0.9422 - val_loss: 2.8930 - val_accuracy: 0.3500\n",
      "Epoch 383/500\n",
      "900/900 [==============================] - 0s 297us/sample - loss: 0.1744 - accuracy: 0.9400 - val_loss: 2.9577 - val_accuracy: 0.3600\n",
      "Epoch 384/500\n",
      "900/900 [==============================] - 0s 290us/sample - loss: 0.1543 - accuracy: 0.9478 - val_loss: 2.9196 - val_accuracy: 0.3700\n",
      "Epoch 385/500\n",
      "900/900 [==============================] - 0s 315us/sample - loss: 0.1753 - accuracy: 0.9389 - val_loss: 2.8787 - val_accuracy: 0.3700\n",
      "Epoch 386/500\n",
      "900/900 [==============================] - 0s 389us/sample - loss: 0.1668 - accuracy: 0.9400 - val_loss: 2.9268 - val_accuracy: 0.3600\n",
      "Epoch 387/500\n",
      "900/900 [==============================] - 0s 409us/sample - loss: 0.1517 - accuracy: 0.9478 - val_loss: 2.9556 - val_accuracy: 0.3700\n",
      "Epoch 388/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.1819 - accuracy: 0.9389 - val_loss: 2.9020 - val_accuracy: 0.3800\n",
      "Epoch 389/500\n",
      "900/900 [==============================] - 0s 284us/sample - loss: 0.1635 - accuracy: 0.9356 - val_loss: 2.8870 - val_accuracy: 0.3800\n",
      "Epoch 390/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.1448 - accuracy: 0.9533 - val_loss: 2.8280 - val_accuracy: 0.3800\n",
      "Epoch 391/500\n",
      "900/900 [==============================] - 0s 301us/sample - loss: 0.1762 - accuracy: 0.9422 - val_loss: 2.8008 - val_accuracy: 0.3800\n",
      "Epoch 392/500\n",
      "900/900 [==============================] - 0s 289us/sample - loss: 0.1601 - accuracy: 0.9422 - val_loss: 2.9430 - val_accuracy: 0.3500\n",
      "Epoch 393/500\n",
      "900/900 [==============================] - 0s 283us/sample - loss: 0.1640 - accuracy: 0.9389 - val_loss: 2.9399 - val_accuracy: 0.3800\n",
      "Epoch 394/500\n",
      "900/900 [==============================] - 0s 285us/sample - loss: 0.2050 - accuracy: 0.9300 - val_loss: 2.8354 - val_accuracy: 0.3800\n",
      "Epoch 395/500\n",
      "900/900 [==============================] - 0s 329us/sample - loss: 0.1881 - accuracy: 0.9400 - val_loss: 2.7171 - val_accuracy: 0.4000\n",
      "Epoch 396/500\n",
      "900/900 [==============================] - 0s 281us/sample - loss: 0.1423 - accuracy: 0.9556 - val_loss: 2.7159 - val_accuracy: 0.4300\n",
      "Epoch 397/500\n",
      "900/900 [==============================] - 0s 289us/sample - loss: 0.1436 - accuracy: 0.9511 - val_loss: 2.6593 - val_accuracy: 0.4000\n",
      "Epoch 398/500\n",
      "900/900 [==============================] - 0s 298us/sample - loss: 0.1694 - accuracy: 0.9356 - val_loss: 2.6540 - val_accuracy: 0.3900\n",
      "Epoch 399/500\n",
      "900/900 [==============================] - 0s 286us/sample - loss: 0.1952 - accuracy: 0.9333 - val_loss: 2.6712 - val_accuracy: 0.4100\n",
      "Epoch 400/500\n",
      "900/900 [==============================] - 0s 372us/sample - loss: 0.1831 - accuracy: 0.9311 - val_loss: 2.6212 - val_accuracy: 0.4100\n",
      "Epoch 401/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.1687 - accuracy: 0.9444 - val_loss: 2.7044 - val_accuracy: 0.3700\n",
      "Epoch 402/500\n",
      "900/900 [==============================] - 0s 288us/sample - loss: 0.1935 - accuracy: 0.9322 - val_loss: 2.8270 - val_accuracy: 0.3600\n",
      "Epoch 403/500\n",
      "900/900 [==============================] - 0s 355us/sample - loss: 0.1781 - accuracy: 0.9444 - val_loss: 2.8322 - val_accuracy: 0.3800\n",
      "Epoch 404/500\n",
      "900/900 [==============================] - 0s 354us/sample - loss: 0.1478 - accuracy: 0.9478 - val_loss: 2.8312 - val_accuracy: 0.3900\n",
      "Epoch 405/500\n",
      "900/900 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.94 - 0s 463us/sample - loss: 0.1619 - accuracy: 0.9444 - val_loss: 2.9468 - val_accuracy: 0.3800\n",
      "Epoch 406/500\n",
      "900/900 [==============================] - 0s 294us/sample - loss: 0.1809 - accuracy: 0.9267 - val_loss: 2.8963 - val_accuracy: 0.4000\n",
      "Epoch 407/500\n",
      "900/900 [==============================] - 0s 296us/sample - loss: 0.2036 - accuracy: 0.9389 - val_loss: 2.8765 - val_accuracy: 0.4000\n",
      "Epoch 408/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.1610 - accuracy: 0.9444 - val_loss: 3.0404 - val_accuracy: 0.3900\n",
      "Epoch 409/500\n",
      "900/900 [==============================] - 0s 290us/sample - loss: 0.1764 - accuracy: 0.9400 - val_loss: 3.0361 - val_accuracy: 0.3400\n",
      "Epoch 410/500\n",
      "900/900 [==============================] - 0s 300us/sample - loss: 0.1845 - accuracy: 0.9311 - val_loss: 2.9588 - val_accuracy: 0.3400\n",
      "Epoch 411/500\n",
      "900/900 [==============================] - 0s 305us/sample - loss: 0.1960 - accuracy: 0.9333 - val_loss: 2.8205 - val_accuracy: 0.3300\n",
      "Epoch 412/500\n",
      "900/900 [==============================] - 0s 304us/sample - loss: 0.1836 - accuracy: 0.9389 - val_loss: 2.7713 - val_accuracy: 0.3400\n",
      "Epoch 413/500\n",
      "900/900 [==============================] - 0s 380us/sample - loss: 0.1929 - accuracy: 0.9356 - val_loss: 2.7211 - val_accuracy: 0.3500\n",
      "Epoch 414/500\n",
      "900/900 [==============================] - 0s 320us/sample - loss: 0.1879 - accuracy: 0.9389 - val_loss: 2.7566 - val_accuracy: 0.3700\n",
      "Epoch 415/500\n",
      "900/900 [==============================] - 0s 298us/sample - loss: 0.1593 - accuracy: 0.9478 - val_loss: 2.7273 - val_accuracy: 0.3800\n",
      "Epoch 416/500\n",
      "900/900 [==============================] - 0s 299us/sample - loss: 0.1687 - accuracy: 0.9333 - val_loss: 2.7565 - val_accuracy: 0.3800\n",
      "Epoch 417/500\n",
      "900/900 [==============================] - 0s 345us/sample - loss: 0.1861 - accuracy: 0.9367 - val_loss: 2.5893 - val_accuracy: 0.4200\n",
      "Epoch 418/500\n",
      "900/900 [==============================] - 0s 303us/sample - loss: 0.1810 - accuracy: 0.9278 - val_loss: 2.6328 - val_accuracy: 0.4100\n",
      "Epoch 419/500\n",
      "900/900 [==============================] - 0s 318us/sample - loss: 0.1855 - accuracy: 0.9333 - val_loss: 2.7526 - val_accuracy: 0.3800\n",
      "Epoch 420/500\n",
      "900/900 [==============================] - 0s 355us/sample - loss: 0.2225 - accuracy: 0.9233 - val_loss: 2.6339 - val_accuracy: 0.3600\n",
      "Epoch 421/500\n",
      "900/900 [==============================] - 0s 307us/sample - loss: 0.2093 - accuracy: 0.9244 - val_loss: 2.5270 - val_accuracy: 0.3500\n",
      "Epoch 422/500\n",
      "900/900 [==============================] - 0s 299us/sample - loss: 0.2044 - accuracy: 0.9233 - val_loss: 2.4715 - val_accuracy: 0.4200\n",
      "Epoch 423/500\n",
      "900/900 [==============================] - 0s 332us/sample - loss: 0.1995 - accuracy: 0.9300 - val_loss: 2.5262 - val_accuracy: 0.4100\n",
      "Epoch 424/500\n",
      "756/900 [========================>.....] - ETA: 0s - loss: 0.1952 - accuracy: 0.9339"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-24d2f07bac3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# fitting the LSTM model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotalX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m108\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# finding test loss and test accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(120, activation='relu'), input_shape=(None, n_words)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=81, input_dim=100, activation='relu'))\n",
    "\n",
    "model.add(Reshape((9,9,1)))\n",
    "model.add(Conv2D(input_shape=(4,4),filters = 100,kernel_size = (3,4),padding='valid'),)\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fitting the LSTM model\n",
    "model.fit(totalX.reshape(-1,1,n_words), totalY, validation_split=0.1, epochs=500, batch_size=108)\n",
    "\n",
    "# finding test loss and test accuracy\n",
    "# loss_rnn, acc_rnn = model.evaluate(X_test,y_test, verbose=0)\n",
    "# print(loss_rnn, acc_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,input_length = maxLength))\n",
    "# Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "# Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "# The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "model.add(Dense(output_dimen, activation='softmax'))\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "# tbCallBack = TensorBoard(log_dir='./Graph/sentiment_chinese', histogram_freq=0,\n",
    "#                             write_graph=True, write_images=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(totalX, totalY, validation_split=0.1, batch_size=32, epochs=20, verbose=1, )\n",
    "model.save('./sample_data/sentiment_chinese_model.HDF5')\n",
    "\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "sentiment_tag = None\n",
    "maxLength = None\n",
    "def loadModel():\n",
    "    global model, sentiment_tag, maxLength\n",
    "    metaData = __loadStuff(\"./sample_data/meta_sentiment_chinese.p\")\n",
    "    maxLength = metaData.get(\"maxLength\")\n",
    "    vocab_size = metaData.get(\"vocab_size\")\n",
    "    output_dimen = metaData.get(\"output_dimen\")\n",
    "    sentiment_tag = metaData.get(\"sentiment_tag\")\n",
    "    embedding_dim = 256\n",
    "    if model is None:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=maxLength))\n",
    "        # Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "        # All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "        model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "        # Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "        model.add(GRU(256, dropout=0.9))\n",
    "        # The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "        model.add(Dense(output_dimen, activation='softmax'))\n",
    "        # We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.load_weights('./sample_data/sentiment_chinese_model.HDF5')\n",
    "        model.summary()\n",
    "    print(\"Model weights loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFeatures(text):\n",
    "    text=Converter('zh-hans').convert(text)\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    text = text.replace(\"\\r\", \"\") \n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    seg_list = list(seg_list)\n",
    "    text = \" \".join(seg_list)\n",
    "    textArray = [text]\n",
    "    input_tokenizer_load = __loadStuff(\"./sample_data/input_tokenizer_chinese.p\")\n",
    "    textArray = np.array(pad_sequences(input_tokenizer_load.texts_to_sequences(textArray), maxlen=maxLength))\n",
    "    return textArray\n",
    "def predictResult(text):\n",
    "    if model is None:\n",
    "        print(\"Please run \\\"loadModel\\\" first.\")\n",
    "        return None\n",
    "    features = findFeatures(text)\n",
    "    predicted = model.predict(features)[0] # we have only one sentence to predict, so take index 0\n",
    "    predicted = np.array(predicted)\n",
    "    probab = predicted.max()\n",
    "    predition = sentiment_tag[predicted.argmax()]\n",
    "    return predition, probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictResult(\"华夏幸福36干进来的，几乎满仓了，后市如何？下周要撤不？华夏幸福36干进来的，几乎满仓了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stock recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201705013063592.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['docid'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201705013065375.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['docid'][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with signal and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
