{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitb9e61b048ecb4ba0b75c9bb993e80432",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Reshape, Conv2D, Activation, MaxPooling2D, Flatten\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_data_label.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Unnamed: 0         docid stock_code   author                      headline  \\\n0           0  2.020000e+14  600340.sh     狼子會投              36元成本，七萬股，目標價50元   \n1           1  2.020000e+14  600340.sh     整齊皓月  華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？   \n2           2  2.020000e+14  600340.sh     思理紅嫩  華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？   \n3           3  2.020000e+14  600211.sh  再見理想123               從集中持股到資產配置，可這樣走   \n4           4  2.020000e+14  002718.sz   genglp                    上市公司當下賣房屬於   \n\n                                             content  \\\n0  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n1  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n2  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n3  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n4  <html>\\n <meta http-equiv=\"Content-Type\" conte...   \n\n                                       clean_content  \\\n0   36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元   \n1  華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？華夏幸福36幹進來的，幾乎滿倉了，後...   \n2  華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？華夏幸福36幹進來的，幾乎滿倉了，後...   \n3  從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又...   \n4                    上市公司當下賣房屬於上市公司當下賣房屬於一是基於但由於業績下滑   \n\n                                  translated_content  label  \n0  36 yuan cost, 70,000 shares, target price 50 y...      2  \n1  Huaxia Fortune 36 came in, almost full of ware...      2  \n2  Huaxia Xingfu 36 came in, almost full of wareh...      2  \n3  From centralized shareholding to asset allocat...      1  \n4  The current house selling of listed companies ...      1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>docid</th>\n      <th>stock_code</th>\n      <th>author</th>\n      <th>headline</th>\n      <th>content</th>\n      <th>clean_content</th>\n      <th>translated_content</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2.020000e+14</td>\n      <td>600340.sh</td>\n      <td>狼子會投</td>\n      <td>36元成本，七萬股，目標價50元</td>\n      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n      <td>36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元36元成本，七萬股，目標價50元</td>\n      <td>36 yuan cost, 70,000 shares, target price 50 y...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2.020000e+14</td>\n      <td>600340.sh</td>\n      <td>整齊皓月</td>\n      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？</td>\n      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何？下周要撤不？華夏幸福36幹進來的，幾乎滿倉了，後...</td>\n      <td>Huaxia Fortune 36 came in, almost full of ware...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2.020000e+14</td>\n      <td>600340.sh</td>\n      <td>思理紅嫩</td>\n      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？</td>\n      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n      <td>華夏幸福36幹進來的，幾乎滿倉了，後市如何，大神指點下？華夏幸福36幹進來的，幾乎滿倉了，後...</td>\n      <td>Huaxia Xingfu 36 came in, almost full of wareh...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2.020000e+14</td>\n      <td>600211.sh</td>\n      <td>再見理想123</td>\n      <td>從集中持股到資產配置，可這樣走</td>\n      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n      <td>從集中持股到資產配置，可這樣走從集中持股到資產配置，可這樣走周四大幅下殺是否另有玄機，周五又...</td>\n      <td>From centralized shareholding to asset allocat...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2.020000e+14</td>\n      <td>002718.sz</td>\n      <td>genglp</td>\n      <td>上市公司當下賣房屬於</td>\n      <td>&lt;html&gt;\\n &lt;meta http-equiv=\"Content-Type\" conte...</td>\n      <td>上市公司當下賣房屬於上市公司當下賣房屬於一是基於但由於業績下滑</td>\n      <td>The current house selling of listed companies ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(('', ' ', '  ', '  ', '    ','   ', '    ', '   ',' s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc, vocab=None):\n",
    "    tokens = word_tokenize(doc)\n",
    "    # keeping only alphabets    \n",
    "    tokens = [re.sub('[^a-zA-Z]', ' ', word) for word in tokens] \n",
    "    # converting to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # removing stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # removing single characters if any\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    if vocab:\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        tokens = ' '.join(tokens)        \n",
    "    return tokens\n",
    "\n",
    "def add_doc_to_vocab(text, vocab):\n",
    "    tokens = clean_doc(text)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df['translated_content']\n",
    "y = df['label']-1\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       ...,\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.]])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a vocabulary of words\n",
    "vocab = Counter()\n",
    "len_train = len(X_train)\n",
    "for i in range(len_train):\n",
    "    text = X_train.iloc[i]\n",
    "    add_doc_to_vocab(text , vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4245\n[('  ', 846), ('market', 340), ('stock', 298), ('    ', 270), ('   ', 208), ('yuan', 201), ('performance', 180), (' s', 179), ('stocks', 178), ('company', 166), ('     ', 164), ('shares', 152), ('companies', 151), ('      ', 144), ('year', 128), ('listed', 117), ('time', 114), ('price', 111), ('first', 104), ('electric', 102)]\n"
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "# print the 20 most common words\n",
    "print(vocab.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing tokens which occur less than 3 times.\n",
    "min_occurance = 2\n",
    "tokens = [k for k,c in vocab.items() if (c >= min_occurance & len(k) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the vocabulary for futute use\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved vocabulary\n",
    "vocab = load_doc('vocab.txt')\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = []\n",
    "for i in range(len_train):\n",
    "    text = X_train.iloc[i]\n",
    "    doc = clean_doc(text, vocab)\n",
    "    train_doc.append(doc)\n",
    "\n",
    "test_doc = []\n",
    "len_test = len(X_test)\n",
    "for i in range(len_test):\n",
    "    text = X_test.iloc[i]\n",
    "    doc = clean_doc(text, vocab)\n",
    "    test_doc.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing indexes where no tokens are present\n",
    "index_train = []\n",
    "for i in range(len(train_doc)):\n",
    "    if len(train_doc[i]) == 0 :\n",
    "        index_train.append(i)\n",
    "    \n",
    "index_test = []\n",
    "for i in range(len(test_doc)):\n",
    "    if len(test_doc[i]) == 0 :\n",
    "        index_test.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X_train shape:  (900, 1272)\nX_test shape:  (100, 1272)\nY_train.shape (900, 3)\nY_test.shape (900, 3)\n"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_doc)\n",
    "\n",
    "X_train = tokenizer.texts_to_matrix(train_doc, mode='binary')\n",
    "X_test = tokenizer.texts_to_matrix(test_doc, mode='binary')\n",
    "n_words = X_test.shape[1]\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_train.shape\", y_train.shape)\n",
    "print(\"Y_test.shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/18\n900/900 [==============================] - 2s 2ms/sample - loss: 1.0756 - acc: 0.5133\nEpoch 2/18\n900/900 [==============================] - 0s 352us/sample - loss: 1.0269 - acc: 0.5522\nEpoch 3/18\n900/900 [==============================] - 0s 389us/sample - loss: 1.0036 - acc: 0.5522\nEpoch 4/18\n900/900 [==============================] - 0s 330us/sample - loss: 0.9952 - acc: 0.5522\nEpoch 5/18\n900/900 [==============================] - 0s 330us/sample - loss: 0.9921 - acc: 0.5522\nEpoch 6/18\n900/900 [==============================] - 0s 351us/sample - loss: 0.9918 - acc: 0.5522\nEpoch 7/18\n900/900 [==============================] - 0s 342us/sample - loss: 0.9918 - acc: 0.5522\nEpoch 8/18\n900/900 [==============================] - 0s 352us/sample - loss: 0.9888 - acc: 0.5522\nEpoch 9/18\n900/900 [==============================] - 0s 340us/sample - loss: 0.9841 - acc: 0.5522\nEpoch 10/18\n900/900 [==============================] - 0s 360us/sample - loss: 0.9762 - acc: 0.5522\nEpoch 11/18\n900/900 [==============================] - 0s 334us/sample - loss: 0.9623 - acc: 0.5522\nEpoch 12/18\n900/900 [==============================] - 0s 339us/sample - loss: 0.9382 - acc: 0.5522\nEpoch 13/18\n900/900 [==============================] - 0s 375us/sample - loss: 0.9018 - acc: 0.5522\nEpoch 14/18\n900/900 [==============================] - 0s 354us/sample - loss: 0.8539 - acc: 0.5522\nEpoch 15/18\n900/900 [==============================] - 0s 330us/sample - loss: 0.7966 - acc: 0.5522\nEpoch 16/18\n900/900 [==============================] - 0s 451us/sample - loss: 0.7410 - acc: 0.6200\nEpoch 17/18\n900/900 [==============================] - 1s 770us/sample - loss: 0.6870 - acc: 0.7156\nEpoch 18/18\n900/900 [==============================] - 1s 784us/sample - loss: 0.6430 - acc: 0.7356\n1.1476779413223266 0.56\n"
    }
   ],
   "source": [
    "# LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(120, activation='relu'), input_shape=(None, n_words)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=81, input_dim=100, activation='relu'))\n",
    "\n",
    "model.add(Reshape((9,9,1)))\n",
    "model.add(Conv2D(input_shape=(4,4),filters = 100,kernel_size = (3,4),padding='valid'),)\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fitting the LSTM model\n",
    "model.fit(X_train.reshape((-1, 1, n_words)), y_train, epochs=18, batch_size=108)\n",
    "\n",
    "# finding test loss and test accuracy\n",
    "loss_rnn, acc_rnn = model.evaluate(X_test.reshape((-1, 1, n_words)), y_test, verbose=0)\n",
    "print(loss_rnn, acc_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test.reshape((-1, 1, n_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 2,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 1]"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "predict_label = []\n",
    "for i in range(len(prediction)):\n",
    "    predict_label.append(prediction[i].argmax())\n",
    "predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}